{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from stable_diffusion2.model.clip_text_embedder import CLIPTextEmbedder\n",
    "from stable_diffusion2.utils.utils import check_device, get_memory_status\n",
    "\n",
    "EMBEDDED_PROMPTS_DIR = os.path.abspath(\"./input/embedded_prompts/\")\n",
    "NULL_PROMPT = \"\"\n",
    "PROMPTS = \"An old photo of a computer scientist\"\n",
    "NUM_IMAGES = 10\n",
    "SEED = 2982\n",
    "NOISE_MULTIPLIER = 0.1\n",
    "\n",
    "os.makedirs(EMBEDDED_PROMPTS_DIR, exist_ok=True)\n",
    "DEVICE = input(\"Set device: 'cuda:i' or 'cpu'\")\n",
    "DEVICE = check_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_save_prompts(prompts: list, null_prompt=NULL_PROMPT):\n",
    "    null_prompt = null_prompt\n",
    "    prompts = prompts\n",
    "\n",
    "    clip_text_embedder = CLIPTextEmbedder(device=check_device())\n",
    "    clip_text_embedder.load_submodels()\n",
    "\n",
    "    null_cond = clip_text_embedder(null_prompt)\n",
    "    torch.save(null_cond, join(EMBEDDED_PROMPTS_DIR, \"null_cond.pt\"))\n",
    "    print(\n",
    "        \"Null prompt embedding saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'null_cond.pt')}\",\n",
    "    )\n",
    "\n",
    "    embedded_prompts = clip_text_embedder(prompts)\n",
    "    torch.save(embedded_prompts, join(EMBEDDED_PROMPTS_DIR, \"embedded_prompts.pt\"))\n",
    "\n",
    "    print(\n",
    "        \"Prompts embeddings saved at: \",\n",
    "        f\"{join(EMBEDDED_PROMPTS_DIR, 'embedded_prompts.pt')}\",\n",
    "    )\n",
    "\n",
    "    get_memory_status()\n",
    "    clip_text_embedder.to(\"cpu\")\n",
    "    del clip_text_embedder\n",
    "    torch.cuda.empty_cache()\n",
    "    get_memory_status()\n",
    "\n",
    "    return embedded_prompts, null_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Falling back to current device.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Null prompt embedding saved at:  c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\embedded_prompts\\null_cond.pt\n",
      "Prompts embeddings saved at:  c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\embedded_prompts\\embedded_prompts.pt\n",
      "Total: 12287 MiB\n",
      "Free: 10376 MiB\n",
      "Used: 1911 MiB\n",
      "Total: 12287 MiB\n",
      "Free: 10522 MiB\n",
      "Used: 1765 MiB\n"
     ]
    }
   ],
   "source": [
    "embedded_prompts, null_cond = embed_and_save_prompts(PROMPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 77, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the embedded prompts\n",
    "embedding_shape = tuple(embedded_prompts.shape)\n",
    "embedding_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1092, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(1.0230, device='cuda:0', grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check mean and std to use the same for the noise generation\n",
    "# one idea is to use one distribution per position (in the 77 positions)\n",
    "# in this case we would check the mean and std along dimension 2\n",
    "# embedded_prompts.mean(dim=2), embedded_prompts.std(dim=2)\n",
    "embedding_mean, embedding_std = embedded_prompts.mean(), embedded_prompts.std()\n",
    "embedding_mean, embedding_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate noise and add to the embedded prompt\n",
    "generator = torch.Generator(device=DEVICE).manual_seed(SEED)\n",
    "noise = torch.normal(\n",
    "    mean=embedding_mean.item(),\n",
    "    std=embedding_std.item(),\n",
    "    size=embedding_shape,\n",
    "    device=DEVICE,\n",
    "    generator=generator,\n",
    ")\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_e = embedded_prompts + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
