{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Test if running on colab\n",
        "import sys\n",
        "COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "\n",
        "if COLAB:\n",
        "  !git clone https://github.com/kk-digital/kcg-ml-sd1p4\n",
        "  kcg = '/content/kcg-ml-sd1p4'\n",
        "else:\n",
        "  kcg = os.getcwd()"
      ],
      "metadata": {
        "id": "lcrNNBlqrw3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The variable pool™ (I know, I'm terrible at structuring notebooks..)\n",
        "\n",
        "# Directories:\n",
        "project_name = \"\"\n",
        "root_dir = \"/content/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "main_dir      = os.path.join(root_dir, \"lora_training\")\n",
        "images_folder = os.path.join(main_dir, \"dataset\", project_name)\n",
        "output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "# LoRa parameters:\n",
        "\n",
        "## Optimizer and token config\n",
        "optimizer = \"AdamW8bit\"\n",
        "optimizer_args = None\n",
        "continue_from_lora = \"\"\n",
        "weighted_captions = False\n",
        "adjust_tags = False\n",
        "keep_tokens_weight = 1.0\n",
        "\n",
        "## Stable Diffusion model to use for training\n",
        "model_filename = \"sd-v1-5-pruned-noema-fp16.safetensors\"\n",
        "model_file = os.path.join(kcg, \"inputs/models\", model_filename)\n",
        "\n",
        "!aria2c \"https://huggingface.co/hollowstrawberry/stable-diffusion-guide/resolve/main/models/sd-v1-5-pruned-noema-fp16.safetensors\" -d /-o {model_file}\n",
        "custom_model_is_based_on_sd2 = False\n",
        "\n",
        "## Dataset parameters\n",
        "resolution = 512\n",
        "flip_aug = False\n",
        "caption_extension = \".txt\"\n",
        "activation_tags = \"1\"\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "## Step parameters (iterations to train for)\n",
        "num_repeats = 10\n",
        "max_train_epochs = 10\n",
        "max_train_steps = None\n",
        "save_every_n_epochs = 1\n",
        "keep_only_last_n_epochs = 10\n",
        "\n",
        "## Learning parameters\n",
        "train_batch_size = 3\n",
        "unet_lr = 5e-4\n",
        "text_encoder_lr = 1e-4\n",
        "lr_scheduler = \"cosine_with_restarts\"\n",
        "lr_scheduler_num_cycles = 3\n",
        "lr_warmup_ratio = 0.05\n",
        "lr_warmup_steps = 0\n",
        "min_snr_gamma_value = 5.0\n",
        "\n",
        "## Network parameters\n",
        "lora_type = \"LoRA\"\n",
        "network_dim = 32\n",
        "network_alpha = round(network_dim/2)\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None"
      ],
      "metadata": {
        "id": "v8BJLp7PGZH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directories\n",
        "import os\n",
        "\n",
        "for dir in (main_dir, deps_dir, repo_dir, log_folder, output_folder, config_folder):\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")"
      ],
      "metadata": {
        "id": "a7pgLKiRg2HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COMMIT = \"5050971ac687dca70ba0486a583d283e8ae324e2\"\n",
        "\n",
        "os.chdir(root_dir)\n",
        "!git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "os.chdir(repo_dir)\n",
        "if COMMIT:\n",
        "  !git reset --hard {COMMIT}\n",
        "\n",
        "if COLAB:\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "  !apt -y update -qq\n",
        "  !pip -q install --upgrade -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "else:\n",
        "  !mv /content/kcg-ml-sd1p4/train/requirements.txt ./\n",
        "  !pip -q install --upgrade -r requirements.txt \n",
        "\n",
        "!apt install -y aria2 -qq\n",
        "\n",
        "# patch kohya for minor stuff\n",
        "if COLAB:\n",
        "  !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "\n",
        "!sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "!sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "!sed -i 's/model_name + \".\"/model_name + \"-{:02d}.\".format(num_train_epochs)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "from accelerate.utils import write_basic_config\n",
        "if not os.path.exists(accelerate_config_file):\n",
        "  write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"  \n",
        "os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIW89MIvg1Ba",
        "outputId": "8b314cc2-2ac3-445b-da85-455beaf88151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/Loras/kohya-trainer' already exists and is not an empty directory.\n",
            "HEAD is now at 5050971 Merge pull request #388 from kohya-ss/dev\n",
            "4 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for library (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from time import time\n",
        "sys.path.append(os.path.join(kcg, \"scripts\"))\n",
        "\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "\n",
        "# Serious code goes here\n",
        "\n",
        "print(\"Done\")\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  config_dict = {\n",
        "    \"additional_network_arguments\": {\n",
        "      \"unet_lr\": unet_lr,\n",
        "      \"text_encoder_lr\": text_encoder_lr,\n",
        "      \"network_dim\": network_dim,\n",
        "      \"network_alpha\": network_alpha,\n",
        "      \"network_module\": network_module,\n",
        "      \"network_args\": network_args,\n",
        "      \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "      \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "      \"learning_rate\": unet_lr,\n",
        "      \"lr_scheduler\": lr_scheduler,\n",
        "      \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "      \"lr_scheduler_power\": None,\n",
        "      \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "      \"optimizer_type\": optimizer,\n",
        "      \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "      \"max_train_steps\": max_train_steps,\n",
        "      \"max_train_epochs\": max_train_epochs,\n",
        "      \"save_every_n_epochs\": save_every_n_epochs,\n",
        "      \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "      \"train_batch_size\": train_batch_size,\n",
        "      \"noise_offset\": None,\n",
        "      \"clip_skip\": 2,\n",
        "      \"min_snr_gamma\": min_snr_gamma_value,\n",
        "      \"weighted_captions\": weighted_captions,\n",
        "      \"seed\": 42,\n",
        "      \"max_token_length\": 225,\n",
        "      \"xformers\": True,\n",
        "      \"lowram\": COLAB,\n",
        "      \"max_data_loader_n_workers\": 8,\n",
        "      \"persistent_data_loader_workers\": True,\n",
        "      \"save_precision\": \"fp16\",\n",
        "      \"mixed_precision\": \"fp16\",\n",
        "      \"output_dir\": output_folder,\n",
        "      \"logging_dir\": log_folder,\n",
        "      \"output_name\": project_name,\n",
        "      \"log_prefix\": project_name,\n",
        "      \"save_state\": False,\n",
        "      \"save_last_n_epochs_state\": None,\n",
        "      \"resume\": None\n",
        "    },\n",
        "    \"model_arguments\": {\n",
        "      \"pretrained_model_name_or_path\": model_file,\n",
        "      \"v2\": custom_model_is_based_on_sd2,\n",
        "      \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "      \"save_model_as\": \"safetensors\",\n",
        "    },\n",
        "    \"dreambooth_arguments\": {\n",
        "      \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "      \"cache_latents\": True,\n",
        "    },\n",
        "  }\n",
        "\n",
        "  for key in config_dict:\n",
        "    if isinstance(config_dict[key], dict):\n",
        "      config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "  with open(config_file, \"w\") as f:\n",
        "    f.write(toml.dumps(config_dict))\n",
        "  print(f\"\\n Config saved to {config_file}\")\n",
        "\n",
        "  dataset_config_dict = {\n",
        "    \"general\": {\n",
        "      \"resolution\": resolution,\n",
        "      \"keep_tokens\": keep_tokens,\n",
        "      \"flip_aug\": flip_aug,\n",
        "      \"caption_extension\": caption_extension,\n",
        "      \"enable_bucket\": True,\n",
        "      \"bucket_reso_steps\": 64,\n",
        "      \"bucket_no_upscale\": False,\n",
        "      \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "      \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "      {\n",
        "        \"subsets\": [\n",
        "          {\n",
        "            \"num_repeats\": num_repeats,\n",
        "            \"image_dir\": images_folder,\n",
        "            \"class_tokens\": None if caption_extension else project_name\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "\n",
        "  for key in dataset_config_dict:\n",
        "    if isinstance(dataset_config_dict[key], dict):\n",
        "      dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "  with open(dataset_config_file, \"w\") as f:\n",
        "    f.write(toml.dumps(dataset_config_dict))\n",
        "  print(f\"Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def main():\n",
        "\n",
        "  create_config()\n",
        "  \n",
        "  print(\"\\nStarting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "  \n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "cnE9MQtVg3EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder = \"/content/drive/MyDrive/Loras\"\n",
        "\n",
        "import os\n",
        "\n",
        "tree = {}\n",
        "exclude = (\"_logs\", \"/output\")\n",
        "for i, (root, dirs, files) in enumerate(os.walk(folder, topdown=True)):\n",
        "  dirs[:] = [d for d in dirs if all(ex not in d for ex in exclude)]\n",
        "  images = len([f for f in files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))])\n",
        "  captions = len([f for f in files if f.lower().endswith(\".txt\")])\n",
        "  others = len(files) - images - captions\n",
        "  path = root[folder.rfind(\"/\")+1:]\n",
        "  tree[path] = None if not images else f\"{images:>4} images | {captions:>4} captions |\"\n",
        "  if tree[path] and others:\n",
        "    tree[path] += f\" {others:>4} other files\"\n",
        "\n",
        "pad = max(len(k) for k in tree)\n",
        "print(\"\\n\".join(f\"📁{k.ljust(pad)} | {v}\" for k, v in tree.items() if v))\n"
      ],
      "metadata": {
        "id": "Qduq71Mwg4jx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}