{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spherical Interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ArNSnrXw459R",
    "outputId": "c7fb24fa-9493-4bf6-9c00-d96e3bdc1a9f",
    "ExecuteTime": {
     "start_time": "2023-06-16T23:39:17.779152Z",
     "end_time": "2023-06-16T23:39:27.437542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers==0.11.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (0.11.1)\r\n",
      "Requirement already satisfied: requests in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (2.30.0)\r\n",
      "Requirement already satisfied: Pillow in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (9.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (3.12.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (2023.6.3)\r\n",
      "Requirement already satisfied: numpy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (1.24.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (0.15.1)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from diffusers==0.11.1) (6.6.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (6.0)\r\n",
      "Requirement already satisfied: fsspec in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.6.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub>=0.10.0->diffusers==0.11.1) (4.65.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from importlib-metadata->diffusers==0.11.1) (3.15.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->diffusers==0.11.1) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->diffusers==0.11.1) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->diffusers==0.11.1) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->diffusers==0.11.1) (2023.5.7)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: transformers in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (4.30.2)\r\n",
      "Requirement already satisfied: scipy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (1.10.1)\r\n",
      "Requirement already satisfied: ftfy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (6.1.1)\r\n",
      "Requirement already satisfied: accelerate in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (0.20.3)\r\n",
      "Requirement already satisfied: requests in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (2.30.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (0.3.1)\r\n",
      "Requirement already satisfied: filelock in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (3.12.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (2023.6.3)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from ftfy) (0.2.6)\r\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from accelerate) (2.0.1)\r\n",
      "Requirement already satisfied: psutil in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from accelerate) (5.9.5)\r\n",
      "Requirement already satisfied: fsspec in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: labml in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (0.4.162)\r\n",
      "Requirement already satisfied: gitpython in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml) (3.1.31)\r\n",
      "Requirement already satisfied: pyyaml in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml) (6.0)\r\n",
      "Requirement already satisfied: numpy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml) (1.24.3)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from gitpython->labml) (4.0.10)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: labml-nn in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (0.4.133)\r\n",
      "Requirement already satisfied: labml-helpers>=0.4.89 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.4.89)\r\n",
      "Requirement already satisfied: torchvision in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.15.2)\r\n",
      "Requirement already satisfied: torch in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (2.0.1)\r\n",
      "Requirement already satisfied: labml>=0.4.158 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.4.162)\r\n",
      "Requirement already satisfied: einops in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.6.1)\r\n",
      "Requirement already satisfied: fairscale in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.4.13)\r\n",
      "Requirement already satisfied: torchtext in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (0.6.0)\r\n",
      "Requirement already satisfied: numpy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml-nn) (1.24.3)\r\n",
      "Requirement already satisfied: pyyaml in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml>=0.4.158->labml-nn) (6.0)\r\n",
      "Requirement already satisfied: gitpython in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from labml>=0.4.158->labml-nn) (3.1.31)\r\n",
      "Requirement already satisfied: filelock in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch->labml-nn) (3.12.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch->labml-nn) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch->labml-nn) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch->labml-nn) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch->labml-nn) (3.1)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torchtext->labml-nn) (0.1.99)\r\n",
      "Requirement already satisfied: requests in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torchtext->labml-nn) (2.30.0)\r\n",
      "Requirement already satisfied: six in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torchtext->labml-nn) (1.16.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torchtext->labml-nn) (4.65.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torchvision->labml-nn) (9.0.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from gitpython->labml>=0.4.158->labml-nn) (4.0.10)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from jinja2->torch->labml-nn) (2.1.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->torchtext->labml-nn) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->torchtext->labml-nn) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->torchtext->labml-nn) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->torchtext->labml-nn) (2023.5.7)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from sympy->torch->labml-nn) (1.3.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.158->labml-nn) (5.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pytorch-lightning in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (2.0.1)\r\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (0.11.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (4.6.3)\r\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (1.24.3)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (4.65.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (0.8.0)\r\n",
      "Requirement already satisfied: packaging>=17.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (6.0)\r\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from pytorch-lightning) (2023.6.0)\r\n",
      "Requirement already satisfied: requests in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.30.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\r\n",
      "Requirement already satisfied: networkx in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\r\n",
      "Requirement already satisfied: filelock in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.12.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.5.7)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: Pillow==9.0.0 in /Users/kenjehofilena/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages (9.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers==0.11.1\n",
    "!pip install transformers scipy ftfy accelerate\n",
    "!pip3 install labml\n",
    "!pip3 install labml-nn\n",
    "!pip3 install pytorch-lightning\n",
    "!pip install Pillow==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Default env type is test\n",
    "# Change this when running on colab or kaggle\n",
    "ENV_TYPE = \"TEST\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print ('[WARNING] CUDA/GPU is not available! Compute-intensive scripts on this notebook will be run on CPU.')\n",
    "    device =  \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pytest\n",
    "if ENV_TYPE == \"TEST\":\n",
    "    model_path = \"/input/models\"\n",
    "    base_directory = \"../\"\n",
    "else:\n",
    "    # clone the repository\n",
    "    !git clone https://github.com/kk-digital/kcg-ml-sd1p4.git\n",
    "\n",
    "    # move to the repo\n",
    "    %cd kcg-ml-sd1p4/\n",
    "\n",
    "    model_path = \"./\"\n",
    "    # Get the current directory\n",
    "    base_directory = os.getcwd()\n",
    "    base_directory = os.path.join(base_directory, 'kcg-ml')\n",
    "    # download model weights\n",
    "    !wget https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\n",
    "\n",
    "# Insert the paths into sys.path\n",
    "sys.path.insert(0, base_directory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-16T23:39:27.443464Z",
     "end_time": "2023-06-16T23:39:27.445773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:46:41.566401Z",
     "iopub.status.busy": "2023-04-05T11:46:41.565969Z",
     "iopub.status.idle": "2023-04-05T11:46:41.586869Z",
     "shell.execute_reply": "2023-04-05T11:46:41.584269Z",
     "shell.execute_reply.started": "2023-04-05T11:46:41.566360Z"
    },
    "id": "CVUMEfU4XhE2",
    "ExecuteTime": {
     "start_time": "2023-06-16T23:39:27.457404Z",
     "end_time": "2023-06-16T23:39:29.155923Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Generate images using stable diffusion with a prompt\n",
    "summary: >\n",
    " Generate images using stable diffusion with a prompt\n",
    "---\n",
    "\n",
    "# Generate images using [stable diffusion](../index.html) with a prompt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from labml import lab, monit\n",
    "from stable_diffusion.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion.sampler.ddim import DDIMSampler\n",
    "from stable_diffusion.sampler.ddpm import DDPMSampler\n",
    "from stable_diffusion.utils.model import load_model, save_images, set_seed\n",
    "\n",
    "\n",
    "class Txt2Img:\n",
    "    \"\"\"\n",
    "    ### Text to image class\n",
    "    \"\"\"\n",
    "    model: LatentDiffusion\n",
    "\n",
    "    def __init__(self, *,\n",
    "                 checkpoint_path: Path,\n",
    "                 sampler_name: str,\n",
    "                 n_steps: int = 50,\n",
    "                 ddim_eta: float = 0.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param checkpoint_path: is the path of the checkpoint\n",
    "        :param sampler_name: is the name of the [sampler](../sampler/index.html)\n",
    "        :param n_steps: is the number of sampling steps\n",
    "        :param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\n",
    "        \"\"\"\n",
    "        # Load [latent diffusion model](../latent_diffusion.html)\n",
    "        self.model = load_model(checkpoint_path)\n",
    "        # Get device\n",
    "        self.device = torch.device(device)\n",
    "        # Move the model to device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize [sampler](../sampler/index.html)\n",
    "        if sampler_name == 'ddim':\n",
    "            self.sampler = DDIMSampler(self.model,\n",
    "                                       n_steps=n_steps,\n",
    "                                       ddim_eta=ddim_eta)\n",
    "        elif sampler_name == 'ddpm':\n",
    "            self.sampler = DDPMSampler(self.model)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, *,\n",
    "                 dest_path: str,\n",
    "                 batch_size: int = 3,\n",
    "                 prompt: str,\n",
    "                 h: int = 512, w: int = 512,\n",
    "                 uncond_scale: float = 7.5,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param dest_path: is the path to store the generated images\n",
    "        :param batch_size: is the number of images to generate in a batch\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        :param h: is the height of the image\n",
    "        :param w: is the width of the image\n",
    "        :param uncond_scale: is the unconditional guidance scale $s$. This is used for\n",
    "            $\\epsilon_\\theta(x_t, c) = s\\epsilon_\\text{cond}(x_t, c) + (s - 1)\\epsilon_\\text{cond}(x_t, c_u)$\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            images = self.model.autoencoder_decode(x)\n",
    "\n",
    "        # Save images\n",
    "        save_images(images, dest_path, 'txt_')\n",
    "\n",
    "    # functions for pipeline\n",
    "    @torch.no_grad()\n",
    "    def generate_text_embeddings(self, prompt, batch_size=4, uncond_scale=7.5):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Make a batch of prompts\n",
    "        prompts = batch_size * [prompt]\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\n",
    "            if uncond_scale != 1.0:\n",
    "                un_cond = self.model.get_text_conditioning(batch_size * [\"\"])\n",
    "            else:\n",
    "                un_cond = None\n",
    "            # Get the prompt embeddings\n",
    "            cond = self.model.get_text_conditioning(prompts)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return cond, un_cond\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_latent_space(self, cond, un_cond, batch_size=4, uncond_scale=7.5, h=512, w=512):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # Number of channels in the image\n",
    "        c = 4\n",
    "        # Image to latent space resolution reduction\n",
    "        f = 8\n",
    "\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # [Sample in the latent space](../sampler/index.html).\n",
    "            # `x` will be of shape `[batch_size, c, h / f, w / f]`\n",
    "            x = self.sampler.sample(cond=cond,\n",
    "                                    shape=[batch_size, c, h // f, w // f],\n",
    "                                    uncond_scale=uncond_scale,\n",
    "                                    uncond_cond=un_cond)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_image(self, x):\n",
    "        \"\"\"\n",
    "        :param prompt: is the prompt to generate images with\n",
    "        \"\"\"\n",
    "        # AMP auto casting\n",
    "        with torch.autocast(device):\n",
    "            # Decode the image from the [autoencoder](../model/autoencoder.html)\n",
    "            image = self.model.autoencoder_decode(x)\n",
    "        \n",
    "        # return the embeddings\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S5TZk3FGsvAf",
    "outputId": "fea68e95-987a-4595-ba67-e551d4d90c45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "HTML(value='<pre  style=\"overflow-x: scroll;\">Initialize autoencoder...</pre>')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4188a4b2ed4143dbadfbdd3cd77d4e5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m7.5\u001B[39m\n\u001B[1;32m      6\u001B[0m uncond_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m7.5\u001B[39m\n\u001B[0;32m----> 7\u001B[0m txt2img \u001B[38;5;241m=\u001B[39m \u001B[43mTxt2Img\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msd-v1-4.ckpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                      \u001B[49m\u001B[43msampler_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampler_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[4], line 42\u001B[0m, in \u001B[0;36mTxt2Img.__init__\u001B[0;34m(self, checkpoint_path, sampler_name, n_steps, ddim_eta)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m:param checkpoint_path: is the path of the checkpoint\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03m:param sampler_name: is the name of the [sampler](../sampler/index.html)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m:param n_steps: is the number of sampling steps\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m:param ddim_eta: is the [DDIM sampling](../sampler/ddim.html) $\\eta$ constant\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Load [latent diffusion model](../latent_diffusion.html)\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Get device\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/stable_diffusion/utils/model.py:74\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(path, device)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# Initialize the U-Net\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m monit\u001B[38;5;241m.\u001B[39msection(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInitialize U-Net\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m---> 74\u001B[0m     unet_model \u001B[38;5;241m=\u001B[39m \u001B[43mUNetModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m320\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mattention_levels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_res_blocks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mchannel_multipliers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mn_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mtf_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m                           \u001B[49m\u001B[43md_cond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# Initialize the Latent Diffusion model\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m monit\u001B[38;5;241m.\u001B[39msection(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInitialize Latent Diffusion model\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/stable_diffusion/model/unet.py:117\u001B[0m, in \u001B[0;36mUNetModel.__init__\u001B[0;34m(self, in_channels, out_channels, channels, n_res_blocks, attention_levels, channel_multipliers, n_heads, tf_layers, d_cond)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mrange\u001B[39m(levels)):\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;66;03m# Add the residual blocks and attentions\u001B[39;00m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_res_blocks \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;66;03m# Residual block maps from previous number of channels plus the\u001B[39;00m\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;66;03m# skip connections from the input half of U-Net to the number of\u001B[39;00m\n\u001B[1;32m    116\u001B[0m         \u001B[38;5;66;03m# channels in the current level.\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m         layers \u001B[38;5;241m=\u001B[39m [\u001B[43mResBlock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchannels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minput_block_channels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md_time_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannels_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m    118\u001B[0m         channels \u001B[38;5;241m=\u001B[39m channels_list[i]\n\u001B[1;32m    119\u001B[0m         \u001B[38;5;66;03m# Add transformer\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/stable_diffusion/model/unet.py:279\u001B[0m, in \u001B[0;36mResBlock.__init__\u001B[0;34m(self, channels, d_t_emb, out_channels)\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb_layers \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m    271\u001B[0m     nn\u001B[38;5;241m.\u001B[39mSiLU(),\n\u001B[1;32m    272\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(d_t_emb, out_channels),\n\u001B[1;32m    273\u001B[0m )\n\u001B[1;32m    274\u001B[0m \u001B[38;5;66;03m# Final convolution layer\u001B[39;00m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_layers \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m    276\u001B[0m     normalization(out_channels),\n\u001B[1;32m    277\u001B[0m     nn\u001B[38;5;241m.\u001B[39mSiLU(),\n\u001B[1;32m    278\u001B[0m     nn\u001B[38;5;241m.\u001B[39mDropout(\u001B[38;5;241m0.\u001B[39m),\n\u001B[0;32m--> 279\u001B[0m     \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mConv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m )\n\u001B[1;32m    282\u001B[0m \u001B[38;5;66;03m# `channels` to `out_channels` mapping layer for residual connection\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out_channels \u001B[38;5;241m==\u001B[39m channels:\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:450\u001B[0m, in \u001B[0;36mConv2d.__init__\u001B[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[1;32m    448\u001B[0m padding_ \u001B[38;5;241m=\u001B[39m padding \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(padding, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m _pair(padding)\n\u001B[1;32m    449\u001B[0m dilation_ \u001B[38;5;241m=\u001B[39m _pair(dilation)\n\u001B[0;32m--> 450\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    451\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    452\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pair\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:144\u001B[0m, in \u001B[0;36m_ConvNd.__init__\u001B[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_parameter(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:150\u001B[0m, in \u001B[0;36m_ConvNd.reset_parameters\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset_parameters\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001B[39;00m\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;66;03m# uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)\u001B[39;00m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;66;03m# For more details see: https://github.com/pytorch/pytorch/issues/15314#issuecomment-477448573\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m     \u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkaiming_uniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    152\u001B[0m         fan_in, _ \u001B[38;5;241m=\u001B[39m init\u001B[38;5;241m.\u001B[39m_calculate_fan_in_and_fan_out(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight)\n",
      "File \u001B[0;32m~/Desktop/Work/GoWorkSpace/src/github.com/kk-digital/kcg-ml-sd1p4/venv/lib/python3.10/site-packages/torch/nn/init.py:412\u001B[0m, in \u001B[0;36mkaiming_uniform_\u001B[0;34m(tensor, a, mode, nonlinearity)\u001B[0m\n\u001B[1;32m    410\u001B[0m bound \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m3.0\u001B[39m) \u001B[38;5;241m*\u001B[39m std  \u001B[38;5;66;03m# Calculate uniform bounds from standard deviation\u001B[39;00m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbound\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# create an instance of the class\n",
    "sampler_name = 'ddim'\n",
    "steps = 50\n",
    "batch_size = 4\n",
    "scale = 7.5\n",
    "uncond_scale = 7.5\n",
    "txt2img = Txt2Img(checkpoint_path=os.path.join(model_path, 'sd-v1-4.ckpt'),\n",
    "                      sampler_name=sampler_name,\n",
    "                      n_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:45:41.184626Z",
     "iopub.status.busy": "2023-04-05T11:45:41.183995Z",
     "iopub.status.idle": "2023-04-05T11:45:43.161687Z",
     "shell.execute_reply": "2023-04-05T11:45:43.160653Z",
     "shell.execute_reply.started": "2023-04-05T11:45:41.184586Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt1 = \"a photograph of an astronaut riding harley davidson\"\n",
    "prompt2 = \"a photograph of an astronaut surfing\"\n",
    "\n",
    "\n",
    "# Get the embeddings for both prompts\n",
    "embeddings1 = txt2img.generate_text_embeddings(prompt=prompt1, batch_size=batch_size, uncond_scale=uncond_scale)\n",
    "embeddings2 = txt2img.generate_text_embeddings(prompt=prompt2, batch_size=batch_size, uncond_scale=uncond_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:50:20.167815Z",
     "iopub.status.busy": "2023-04-05T11:50:20.167080Z",
     "iopub.status.idle": "2023-04-05T11:52:13.844940Z",
     "shell.execute_reply": "2023-04-05T11:52:13.842956Z",
     "shell.execute_reply.started": "2023-04-05T11:50:20.167774Z"
    },
    "id": "8tyZtbFKuAGu"
   },
   "outputs": [],
   "source": [
    "#Get the latents fro the both prompts\n",
    "\n",
    "latent_space1 = txt2img.generate_latent_space(cond=embeddings1[0], un_cond=embeddings1[1], batch_size=batch_size, uncond_scale=uncond_scale, h=512, w=512)\n",
    "latent_space2 = txt2img.generate_latent_space(cond=embeddings2[0], un_cond=embeddings2[1], batch_size=batch_size, uncond_scale=uncond_scale, h=512, w=512)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  InterpolateLatentSpherical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:18.248687Z",
     "iopub.status.busy": "2023-04-05T11:53:18.247954Z",
     "iopub.status.idle": "2023-04-05T11:53:18.256017Z",
     "shell.execute_reply": "2023-04-05T11:53:18.254879Z",
     "shell.execute_reply.started": "2023-04-05T11:53:18.248646Z"
    }
   },
   "outputs": [],
   "source": [
    "def interpolate_latent_spherical(latent1, latent2, alpha):\n",
    "    latent1_flat, latent2_flat = latent1.view(latent1.shape[0], -1), latent2.view(latent2.shape[0], -1)\n",
    "    latent1_norm, latent2_norm = torch.norm(latent1_flat, dim=-1), torch.norm(latent2_flat, dim=-1)\n",
    "    dot_product = torch.sum(latent1_flat * latent2_flat, dim=-1) / (latent1_norm * latent2_norm)\n",
    "    \n",
    "    omega, sin_omega = torch.acos(dot_product), torch.sin(torch.acos(dot_product))\n",
    "    alpha_weights, one_minus_alpha_weights = torch.sin(alpha * omega) / sin_omega, torch.sin((1 - alpha) * omega) / sin_omega\n",
    "    \n",
    "    return latent1 * one_minus_alpha_weights[:, None, None, None] + latent2 * alpha_weights[:, None, None, None]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:20.210388Z",
     "iopub.status.busy": "2023-04-05T11:53:20.209638Z",
     "iopub.status.idle": "2023-04-05T11:53:20.215391Z",
     "shell.execute_reply": "2023-04-05T11:53:20.214245Z",
     "shell.execute_reply.started": "2023-04-05T11:53:20.210349Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_image(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T11:53:23.169491Z",
     "iopub.status.busy": "2023-04-05T11:53:23.169112Z",
     "iopub.status.idle": "2023-04-05T11:53:49.473772Z",
     "shell.execute_reply": "2023-04-05T11:53:49.472694Z",
     "shell.execute_reply.started": "2023-04-05T11:53:23.169455Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "output_dir = './output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "num_interpolations = 2\n",
    "alphas = torch.tensor([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]).to(txt2img.device)\n",
    "for i, alpha in enumerate(alphas):\n",
    "    interpolated_latent_spherical = interpolate_latent_spherical(latent_space1, latent_space2, alpha)\n",
    "        \n",
    "    # Generate images from the interpolated latent\n",
    "    with torch.no_grad():\n",
    "        images = txt2img.model.autoencoder_decode(interpolated_latent_spherical)\n",
    "        file_name = f\"interpolated_00000{i}.jpeg\"\n",
    "        img_path = os.path.join(output_dir, file_name)\n",
    "        save_images(images, img_path)\n",
    "\n",
    "    # Display the first and last images\n",
    "    display_image(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deAXKGQP3_ix"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
