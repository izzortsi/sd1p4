{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import safetensors as st\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.utils.model import *\n",
    "from stable_diffusion.utils.utils import get_device\n",
    "from stable_diffusion.utils.model import initialize_latent_diffusion\n",
    "from stable_diffusion.constants import ModelsPathTree\n",
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/izzortsi/.cloned/kcg-ml-sd1p4/input/models/clip/text_embedder.safetensors'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt = ModelsPathTree()\n",
    "DEVICE = get_device()\n",
    "pt.embedder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">autoencoder initialization...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'text_projection.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    }
   ],
   "source": [
    "ld = initialize_latent_diffusion(pt.sd_default_model_checkpoint_path, device=DEVICE, force_submodels_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion = StableDiffusion(device=DEVICE, model = ld, ddim_steps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stable_diffusion\u001b[39m.\u001b[39;49mgenerate_images(prompt \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mA greenish smiling salsage\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cloned/kcg-ml-sd1p4/stable_diffusion/stable_diffusion.py:327\u001b[0m, in \u001b[0;36mStableDiffusion.generate_images\u001b[0;34m(self, seed, batch_size, prompt, h, w, uncond_scale, low_vram, noise_fn, temperature)\u001b[0m\n\u001b[1;32m    321\u001b[0m un_cond, cond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_text_conditioning(\n\u001b[1;32m    322\u001b[0m     uncond_scale, prompts, batch_size\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    324\u001b[0m \u001b[39m# [Sample in the latent space](../sampler/index.html).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39m# `x` will be of shape `[batch_size, c, h / f, w / f]`\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m# with section(\"sampling\"):\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    328\u001b[0m     cond\u001b[39m=\u001b[39mcond,\n\u001b[1;32m    329\u001b[0m     shape\u001b[39m=\u001b[39m[batch_size, c, h \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f, w \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f],\n\u001b[1;32m    330\u001b[0m     uncond_scale\u001b[39m=\u001b[39muncond_scale,\n\u001b[1;32m    331\u001b[0m     uncond_cond\u001b[39m=\u001b[39mun_cond,\n\u001b[1;32m    332\u001b[0m     noise_fn\u001b[39m=\u001b[39mnoise_fn,\n\u001b[1;32m    333\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    334\u001b[0m )\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode_image(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "stable_diffusion.generate_images(prompt = 'A greenish smiling salsage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safetensors.torch.load_model(stable_diffusion.model, pt.sd_default_model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">encoder initialization...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    }
   ],
   "source": [
    "encoder = initialize_encoder(device=DEVICE)\n",
    "ld.autoencoder = initialize_autoencoder(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">autoencoder initialization...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    }
   ],
   "source": [
    "vae = initialize_autoencoder(device=DEVICE, force_submodels_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): DownSample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): DownSample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): DownSample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (3): Module(\n",
       "        (block): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): Identity()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsample): Identity()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsample): UpSample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x Module(\n",
       "        (block): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (upsample): UpSample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Using device  cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n",
      "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
      "INFO: Device given. Using device cpu.\n",
      "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.\n",
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): UNetWrapper()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "stable_diffusion = StableDiffusion(\n",
    "        device=device\n",
    "    )\n",
    "stable_diffusion.quick_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_diffusion.model.clip_text_embedder.clip_text_embedder import CLIPTextEmbedder\n",
    "from typing import List\n",
    "\n",
    "from torch import nn\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "\n",
    "class CLIPTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    ## CLIP Text Embedder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_model_path, tokenizer_path, device=\"cuda:0\", max_length: int = 77):\n",
    "        \"\"\"\n",
    "        :param version: is the model version\n",
    "        :param device: is the device\n",
    "        :param max_length: is the max length of the tokenized prompt\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = get_device(device)\n",
    "        # Load the tokenizer\n",
    "        self.tokenizer = None\n",
    "        # Load the CLIP transformer\n",
    "        self.transformer = None\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, prompts: List[str]):\n",
    "        \"\"\"\n",
    "        :param prompts: are the list of prompts to embed\n",
    "        \"\"\"\n",
    "        # Tokenize the prompts\n",
    "        batch_encoding = self.tokenizer(prompts, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        # Get token ids\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        # Get CLIP embeddings\n",
    "        return self.transformer(input_ids=tokens).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "It looks like the config file at '/home/izzortsi/.cloned/kcg-ml-sd1p4/input/models/clip/text_embedder.safetensors' is not a valid JSON file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/configuration_utils.py:659\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[0;32m--> 659\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m    660\u001b[0m     config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m commit_hash\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/configuration_utils.py:750\u001b[0m, in \u001b[0;36mPretrainedConfig._dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(json_file, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n\u001b[0;32m--> 750\u001b[0m     text \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m    751\u001b[0m \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x91 in position 23955: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# text_embedder = CLIPTextEmbedder(pt.embedder_path, pt.tokenizer_path, device=)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text_model \u001b[39m=\u001b[39m CLIPTextModel\u001b[39m.\u001b[39;49mfrom_pretrained(pt\u001b[39m.\u001b[39;49mtransformer_path)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/modeling_utils.py:2251\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2250\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2251\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2252\u001b[0m         config_path,\n\u001b[1;32m   2253\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2254\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2255\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2256\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2257\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2258\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2259\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2260\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2261\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2262\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2263\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2264\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2265\u001b[0m     )\n\u001b[1;32m   2266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2267\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/models/clip/configuration_clip.py:130\u001b[0m, in \u001b[0;36mCLIPTextConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 130\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    132\u001b[0m     \u001b[39m# get the text config dict if we are loading from CLIPConfig\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m config_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    573\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    576\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/transformers/configuration_utils.py:662\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m     config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m commit_hash\n\u001b[1;32m    661\u001b[0m \u001b[39mexcept\u001b[39;00m (json\u001b[39m.\u001b[39mJSONDecodeError, \u001b[39mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    663\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIt looks like the config file at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mresolved_config_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not a valid JSON file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    664\u001b[0m     )\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m is_local:\n\u001b[1;32m    667\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading configuration file \u001b[39m\u001b[39m{\u001b[39;00mresolved_config_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: It looks like the config file at '/home/izzortsi/.cloned/kcg-ml-sd1p4/input/models/clip/text_embedder.safetensors' is not a valid JSON file."
     ]
    }
   ],
   "source": [
    "# text_embedder = CLIPTextEmbedder(pt.embedder_path, pt.tokenizer_path, device=)\n",
    "text_model = CLIPTextModel.from_pretrained(pt.transformer_path).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m load_model(stable_diffusion\u001b[39m.\u001b[39mmodel, pt\u001b[39m.\u001b[39mcheckpoint_path, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = load_model(stable_diffusion.model, pt.checkpoint_path, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alphas_cumprod',\n",
       " 'alphas_cumprod_prev',\n",
       " 'betas',\n",
       " 'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight',\n",
       " 'cond_stage_model.transformer.text_model.embeddings.position_ids',\n",
       " 'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias',\n",
       " 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'cond_stage_model.transformer.text_model.final_layer_norm.bias',\n",
       " 'cond_stage_model.transformer.text_model.final_layer_norm.weight',\n",
       " 'first_stage_model.decoder.conv_in.bias',\n",
       " 'first_stage_model.decoder.conv_in.weight',\n",
       " 'first_stage_model.decoder.conv_out.bias',\n",
       " 'first_stage_model.decoder.conv_out.weight',\n",
       " 'first_stage_model.decoder.mid.attn_1.k.bias',\n",
       " 'first_stage_model.decoder.mid.attn_1.k.weight',\n",
       " 'first_stage_model.decoder.mid.attn_1.norm.bias',\n",
       " 'first_stage_model.decoder.mid.attn_1.norm.weight',\n",
       " 'first_stage_model.decoder.mid.attn_1.proj_out.bias',\n",
       " 'first_stage_model.decoder.mid.attn_1.proj_out.weight',\n",
       " 'first_stage_model.decoder.mid.attn_1.q.bias',\n",
       " 'first_stage_model.decoder.mid.attn_1.q.weight',\n",
       " 'first_stage_model.decoder.mid.attn_1.v.bias',\n",
       " 'first_stage_model.decoder.mid.attn_1.v.weight',\n",
       " 'first_stage_model.decoder.mid.block_1.conv1.bias',\n",
       " 'first_stage_model.decoder.mid.block_1.conv1.weight',\n",
       " 'first_stage_model.decoder.mid.block_1.conv2.bias',\n",
       " 'first_stage_model.decoder.mid.block_1.conv2.weight',\n",
       " 'first_stage_model.decoder.mid.block_1.norm1.bias',\n",
       " 'first_stage_model.decoder.mid.block_1.norm1.weight',\n",
       " 'first_stage_model.decoder.mid.block_1.norm2.bias',\n",
       " 'first_stage_model.decoder.mid.block_1.norm2.weight',\n",
       " 'first_stage_model.decoder.mid.block_2.conv1.bias',\n",
       " 'first_stage_model.decoder.mid.block_2.conv1.weight',\n",
       " 'first_stage_model.decoder.mid.block_2.conv2.bias',\n",
       " 'first_stage_model.decoder.mid.block_2.conv2.weight',\n",
       " 'first_stage_model.decoder.mid.block_2.norm1.bias',\n",
       " 'first_stage_model.decoder.mid.block_2.norm1.weight',\n",
       " 'first_stage_model.decoder.mid.block_2.norm2.bias',\n",
       " 'first_stage_model.decoder.mid.block_2.norm2.weight',\n",
       " 'first_stage_model.decoder.norm_out.bias',\n",
       " 'first_stage_model.decoder.norm_out.weight',\n",
       " 'first_stage_model.decoder.up.0.block.0.conv1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.0.conv1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.0.conv2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.0.conv2.weight',\n",
       " 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias',\n",
       " 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight',\n",
       " 'first_stage_model.decoder.up.0.block.0.norm1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.0.norm1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.0.norm2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.0.norm2.weight',\n",
       " 'first_stage_model.decoder.up.0.block.1.conv1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.1.conv1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.1.conv2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.1.conv2.weight',\n",
       " 'first_stage_model.decoder.up.0.block.1.norm1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.1.norm1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.1.norm2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.1.norm2.weight',\n",
       " 'first_stage_model.decoder.up.0.block.2.conv1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.2.conv1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.2.conv2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.2.conv2.weight',\n",
       " 'first_stage_model.decoder.up.0.block.2.norm1.bias',\n",
       " 'first_stage_model.decoder.up.0.block.2.norm1.weight',\n",
       " 'first_stage_model.decoder.up.0.block.2.norm2.bias',\n",
       " 'first_stage_model.decoder.up.0.block.2.norm2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.0.conv1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.0.conv1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.0.conv2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.0.conv2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias',\n",
       " 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight',\n",
       " 'first_stage_model.decoder.up.1.block.0.norm1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.0.norm1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.0.norm2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.0.norm2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.1.conv1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.1.conv1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.1.conv2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.1.conv2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.1.norm1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.1.norm1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.1.norm2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.1.norm2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.2.conv1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.2.conv1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.2.conv2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.2.conv2.weight',\n",
       " 'first_stage_model.decoder.up.1.block.2.norm1.bias',\n",
       " 'first_stage_model.decoder.up.1.block.2.norm1.weight',\n",
       " 'first_stage_model.decoder.up.1.block.2.norm2.bias',\n",
       " 'first_stage_model.decoder.up.1.block.2.norm2.weight',\n",
       " 'first_stage_model.decoder.up.1.upsample.conv.bias',\n",
       " 'first_stage_model.decoder.up.1.upsample.conv.weight',\n",
       " 'first_stage_model.decoder.up.2.block.0.conv1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.0.conv1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.0.conv2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.0.conv2.weight',\n",
       " 'first_stage_model.decoder.up.2.block.0.norm1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.0.norm1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.0.norm2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.0.norm2.weight',\n",
       " 'first_stage_model.decoder.up.2.block.1.conv1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.1.conv1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.1.conv2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.1.conv2.weight',\n",
       " 'first_stage_model.decoder.up.2.block.1.norm1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.1.norm1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.1.norm2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.1.norm2.weight',\n",
       " 'first_stage_model.decoder.up.2.block.2.conv1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.2.conv1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.2.conv2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.2.conv2.weight',\n",
       " 'first_stage_model.decoder.up.2.block.2.norm1.bias',\n",
       " 'first_stage_model.decoder.up.2.block.2.norm1.weight',\n",
       " 'first_stage_model.decoder.up.2.block.2.norm2.bias',\n",
       " 'first_stage_model.decoder.up.2.block.2.norm2.weight',\n",
       " 'first_stage_model.decoder.up.2.upsample.conv.bias',\n",
       " 'first_stage_model.decoder.up.2.upsample.conv.weight',\n",
       " 'first_stage_model.decoder.up.3.block.0.conv1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.0.conv1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.0.conv2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.0.conv2.weight',\n",
       " 'first_stage_model.decoder.up.3.block.0.norm1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.0.norm1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.0.norm2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.0.norm2.weight',\n",
       " 'first_stage_model.decoder.up.3.block.1.conv1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.1.conv1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.1.conv2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.1.conv2.weight',\n",
       " 'first_stage_model.decoder.up.3.block.1.norm1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.1.norm1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.1.norm2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.1.norm2.weight',\n",
       " 'first_stage_model.decoder.up.3.block.2.conv1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.2.conv1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.2.conv2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.2.conv2.weight',\n",
       " 'first_stage_model.decoder.up.3.block.2.norm1.bias',\n",
       " 'first_stage_model.decoder.up.3.block.2.norm1.weight',\n",
       " 'first_stage_model.decoder.up.3.block.2.norm2.bias',\n",
       " 'first_stage_model.decoder.up.3.block.2.norm2.weight',\n",
       " 'first_stage_model.decoder.up.3.upsample.conv.bias',\n",
       " 'first_stage_model.decoder.up.3.upsample.conv.weight',\n",
       " 'first_stage_model.encoder.conv_in.bias',\n",
       " 'first_stage_model.encoder.conv_in.weight',\n",
       " 'first_stage_model.encoder.conv_out.bias',\n",
       " 'first_stage_model.encoder.conv_out.weight',\n",
       " 'first_stage_model.encoder.down.0.block.0.conv1.bias',\n",
       " 'first_stage_model.encoder.down.0.block.0.conv1.weight',\n",
       " 'first_stage_model.encoder.down.0.block.0.conv2.bias',\n",
       " 'first_stage_model.encoder.down.0.block.0.conv2.weight',\n",
       " 'first_stage_model.encoder.down.0.block.0.norm1.bias',\n",
       " 'first_stage_model.encoder.down.0.block.0.norm1.weight',\n",
       " 'first_stage_model.encoder.down.0.block.0.norm2.bias',\n",
       " 'first_stage_model.encoder.down.0.block.0.norm2.weight',\n",
       " 'first_stage_model.encoder.down.0.block.1.conv1.bias',\n",
       " 'first_stage_model.encoder.down.0.block.1.conv1.weight',\n",
       " 'first_stage_model.encoder.down.0.block.1.conv2.bias',\n",
       " 'first_stage_model.encoder.down.0.block.1.conv2.weight',\n",
       " 'first_stage_model.encoder.down.0.block.1.norm1.bias',\n",
       " 'first_stage_model.encoder.down.0.block.1.norm1.weight',\n",
       " 'first_stage_model.encoder.down.0.block.1.norm2.bias',\n",
       " 'first_stage_model.encoder.down.0.block.1.norm2.weight',\n",
       " 'first_stage_model.encoder.down.0.downsample.conv.bias',\n",
       " 'first_stage_model.encoder.down.0.downsample.conv.weight',\n",
       " 'first_stage_model.encoder.down.1.block.0.conv1.bias',\n",
       " 'first_stage_model.encoder.down.1.block.0.conv1.weight',\n",
       " 'first_stage_model.encoder.down.1.block.0.conv2.bias',\n",
       " 'first_stage_model.encoder.down.1.block.0.conv2.weight',\n",
       " 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias',\n",
       " 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight',\n",
       " 'first_stage_model.encoder.down.1.block.0.norm1.bias',\n",
       " 'first_stage_model.encoder.down.1.block.0.norm1.weight',\n",
       " 'first_stage_model.encoder.down.1.block.0.norm2.bias',\n",
       " 'first_stage_model.encoder.down.1.block.0.norm2.weight',\n",
       " 'first_stage_model.encoder.down.1.block.1.conv1.bias',\n",
       " 'first_stage_model.encoder.down.1.block.1.conv1.weight',\n",
       " 'first_stage_model.encoder.down.1.block.1.conv2.bias',\n",
       " 'first_stage_model.encoder.down.1.block.1.conv2.weight',\n",
       " 'first_stage_model.encoder.down.1.block.1.norm1.bias',\n",
       " 'first_stage_model.encoder.down.1.block.1.norm1.weight',\n",
       " 'first_stage_model.encoder.down.1.block.1.norm2.bias',\n",
       " 'first_stage_model.encoder.down.1.block.1.norm2.weight',\n",
       " 'first_stage_model.encoder.down.1.downsample.conv.bias',\n",
       " 'first_stage_model.encoder.down.1.downsample.conv.weight',\n",
       " 'first_stage_model.encoder.down.2.block.0.conv1.bias',\n",
       " 'first_stage_model.encoder.down.2.block.0.conv1.weight',\n",
       " 'first_stage_model.encoder.down.2.block.0.conv2.bias',\n",
       " 'first_stage_model.encoder.down.2.block.0.conv2.weight',\n",
       " 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias',\n",
       " 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight',\n",
       " 'first_stage_model.encoder.down.2.block.0.norm1.bias',\n",
       " 'first_stage_model.encoder.down.2.block.0.norm1.weight',\n",
       " 'first_stage_model.encoder.down.2.block.0.norm2.bias',\n",
       " 'first_stage_model.encoder.down.2.block.0.norm2.weight',\n",
       " 'first_stage_model.encoder.down.2.block.1.conv1.bias',\n",
       " 'first_stage_model.encoder.down.2.block.1.conv1.weight',\n",
       " 'first_stage_model.encoder.down.2.block.1.conv2.bias',\n",
       " 'first_stage_model.encoder.down.2.block.1.conv2.weight',\n",
       " 'first_stage_model.encoder.down.2.block.1.norm1.bias',\n",
       " 'first_stage_model.encoder.down.2.block.1.norm1.weight',\n",
       " 'first_stage_model.encoder.down.2.block.1.norm2.bias',\n",
       " 'first_stage_model.encoder.down.2.block.1.norm2.weight',\n",
       " 'first_stage_model.encoder.down.2.downsample.conv.bias',\n",
       " 'first_stage_model.encoder.down.2.downsample.conv.weight',\n",
       " 'first_stage_model.encoder.down.3.block.0.conv1.bias',\n",
       " 'first_stage_model.encoder.down.3.block.0.conv1.weight',\n",
       " 'first_stage_model.encoder.down.3.block.0.conv2.bias',\n",
       " 'first_stage_model.encoder.down.3.block.0.conv2.weight',\n",
       " 'first_stage_model.encoder.down.3.block.0.norm1.bias',\n",
       " 'first_stage_model.encoder.down.3.block.0.norm1.weight',\n",
       " 'first_stage_model.encoder.down.3.block.0.norm2.bias',\n",
       " 'first_stage_model.encoder.down.3.block.0.norm2.weight',\n",
       " 'first_stage_model.encoder.down.3.block.1.conv1.bias',\n",
       " 'first_stage_model.encoder.down.3.block.1.conv1.weight',\n",
       " 'first_stage_model.encoder.down.3.block.1.conv2.bias',\n",
       " 'first_stage_model.encoder.down.3.block.1.conv2.weight',\n",
       " 'first_stage_model.encoder.down.3.block.1.norm1.bias',\n",
       " 'first_stage_model.encoder.down.3.block.1.norm1.weight',\n",
       " 'first_stage_model.encoder.down.3.block.1.norm2.bias',\n",
       " 'first_stage_model.encoder.down.3.block.1.norm2.weight',\n",
       " 'first_stage_model.encoder.mid.attn_1.k.bias',\n",
       " 'first_stage_model.encoder.mid.attn_1.k.weight',\n",
       " 'first_stage_model.encoder.mid.attn_1.norm.bias',\n",
       " 'first_stage_model.encoder.mid.attn_1.norm.weight',\n",
       " 'first_stage_model.encoder.mid.attn_1.proj_out.bias',\n",
       " 'first_stage_model.encoder.mid.attn_1.proj_out.weight',\n",
       " 'first_stage_model.encoder.mid.attn_1.q.bias',\n",
       " 'first_stage_model.encoder.mid.attn_1.q.weight',\n",
       " 'first_stage_model.encoder.mid.attn_1.v.bias',\n",
       " 'first_stage_model.encoder.mid.attn_1.v.weight',\n",
       " 'first_stage_model.encoder.mid.block_1.conv1.bias',\n",
       " 'first_stage_model.encoder.mid.block_1.conv1.weight',\n",
       " 'first_stage_model.encoder.mid.block_1.conv2.bias',\n",
       " 'first_stage_model.encoder.mid.block_1.conv2.weight',\n",
       " 'first_stage_model.encoder.mid.block_1.norm1.bias',\n",
       " 'first_stage_model.encoder.mid.block_1.norm1.weight',\n",
       " 'first_stage_model.encoder.mid.block_1.norm2.bias',\n",
       " 'first_stage_model.encoder.mid.block_1.norm2.weight',\n",
       " 'first_stage_model.encoder.mid.block_2.conv1.bias',\n",
       " 'first_stage_model.encoder.mid.block_2.conv1.weight',\n",
       " 'first_stage_model.encoder.mid.block_2.conv2.bias',\n",
       " 'first_stage_model.encoder.mid.block_2.conv2.weight',\n",
       " 'first_stage_model.encoder.mid.block_2.norm1.bias',\n",
       " 'first_stage_model.encoder.mid.block_2.norm1.weight',\n",
       " 'first_stage_model.encoder.mid.block_2.norm2.bias',\n",
       " 'first_stage_model.encoder.mid.block_2.norm2.weight',\n",
       " 'first_stage_model.encoder.norm_out.bias',\n",
       " 'first_stage_model.encoder.norm_out.weight',\n",
       " 'first_stage_model.post_quant_conv.bias',\n",
       " 'first_stage_model.post_quant_conv.weight',\n",
       " 'first_stage_model.quant_conv.bias',\n",
       " 'first_stage_model.quant_conv.weight',\n",
       " 'log_one_minus_alphas_cumprod',\n",
       " 'model_ema.decay',\n",
       " 'model_ema.num_updates',\n",
       " 'posterior_log_variance_clipped',\n",
       " 'posterior_mean_coef1',\n",
       " 'posterior_mean_coef2',\n",
       " 'posterior_variance',\n",
       " 'sqrt_alphas_cumprod',\n",
       " 'sqrt_one_minus_alphas_cumprod',\n",
       " 'sqrt_recip_alphas_cumprod',\n",
       " 'sqrt_recipm1_alphas_cumprod',\n",
       " 'model.diffusion_model.input_blocks.0.0.bias',\n",
       " 'model.diffusion_model.input_blocks.0.0.weight',\n",
       " 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.3.0.op.bias',\n",
       " 'model.diffusion_model.input_blocks.3.0.op.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.4.0.skip_connection.bias',\n",
       " 'model.diffusion_model.input_blocks.4.0.skip_connection.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.6.0.op.bias',\n",
       " 'model.diffusion_model.input_blocks.6.0.op.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.7.0.skip_connection.bias',\n",
       " 'model.diffusion_model.input_blocks.7.0.skip_connection.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.norm.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.norm.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.proj_in.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.proj_in.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.proj_out.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.proj_out.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.input_blocks.9.0.op.bias',\n",
       " 'model.diffusion_model.input_blocks.9.0.op.weight',\n",
       " 'model.diffusion_model.middle_block.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.middle_block.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.middle_block.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.middle_block.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.middle_block.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.middle_block.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.middle_block.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.middle_block.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.middle_block.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.middle_block.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.middle_block.1.norm.bias',\n",
       " 'model.diffusion_model.middle_block.1.norm.weight',\n",
       " 'model.diffusion_model.middle_block.1.proj_in.bias',\n",
       " 'model.diffusion_model.middle_block.1.proj_in.weight',\n",
       " 'model.diffusion_model.middle_block.1.proj_out.bias',\n",
       " 'model.diffusion_model.middle_block.1.proj_out.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.middle_block.2.emb_layers.1.bias',\n",
       " 'model.diffusion_model.middle_block.2.emb_layers.1.weight',\n",
       " 'model.diffusion_model.middle_block.2.in_layers.0.bias',\n",
       " 'model.diffusion_model.middle_block.2.in_layers.0.weight',\n",
       " 'model.diffusion_model.middle_block.2.in_layers.2.bias',\n",
       " 'model.diffusion_model.middle_block.2.in_layers.2.weight',\n",
       " 'model.diffusion_model.middle_block.2.out_layers.0.bias',\n",
       " 'model.diffusion_model.middle_block.2.out_layers.0.weight',\n",
       " 'model.diffusion_model.middle_block.2.out_layers.3.bias',\n",
       " 'model.diffusion_model.middle_block.2.out_layers.3.weight',\n",
       " 'model.diffusion_model.out.0.bias',\n",
       " 'model.diffusion_model.out.0.weight',\n",
       " 'model.diffusion_model.out.2.bias',\n",
       " 'model.diffusion_model.out.2.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.0.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.0.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.1.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.1.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.10.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.10.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.norm.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.norm.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.proj_in.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.proj_in.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.proj_out.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.proj_out.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.11.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.11.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.norm.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.norm.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.proj_in.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.proj_in.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.proj_out.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.proj_out.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.2.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.2.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.2.1.conv.bias',\n",
       " 'model.diffusion_model.output_blocks.2.1.conv.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.3.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.3.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.norm.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.norm.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.proj_in.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.proj_in.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.proj_out.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.proj_out.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.4.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.4.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.norm.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.norm.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.proj_in.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.proj_in.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.proj_out.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.proj_out.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.5.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.5.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.norm.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.norm.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.proj_in.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.proj_in.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.proj_out.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.proj_out.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias',\n",
       " 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight',\n",
       " 'model.diffusion_model.output_blocks.5.2.conv.bias',\n",
       " 'model.diffusion_model.output_blocks.5.2.conv.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight',\n",
       " 'model.diffusion_model.output_blocks.6.0.skip_connection.bias',\n",
       " 'model.diffusion_model.output_blocks.6.0.skip_connection.weight',\n",
       " 'model.diffusion_model.output_blocks.6.1.norm.bias',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are running this script without CUDA. It may be very slow.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stable_diffusion\u001b[39m.\u001b[39;49mgenerate_images(prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA cat digging a hole\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/kcg/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cloned/kcg-ml-sd1p4/stable_diffusion/stable_diffusion.py:321\u001b[0m, in \u001b[0;36mStableDiffusion.generate_images\u001b[0;34m(self, seed, batch_size, prompt, h, w, uncond_scale, low_vram, noise_fn, temperature)\u001b[0m\n\u001b[1;32m    318\u001b[0m autocast \u001b[39m=\u001b[39m get_autocast()\n\u001b[1;32m    319\u001b[0m \u001b[39mwith\u001b[39;00m autocast:\n\u001b[1;32m    320\u001b[0m     \u001b[39m# with section(\"getting text cond\"):\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     un_cond, cond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_text_conditioning(\n\u001b[1;32m    322\u001b[0m         uncond_scale, prompts, batch_size\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    324\u001b[0m     \u001b[39m# [Sample in the latent space](../sampler/index.html).\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[39m# `x` will be of shape `[batch_size, c, h / f, w / f]`\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[39m# with section(\"sampling\"):\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39msample(\n\u001b[1;32m    328\u001b[0m         cond\u001b[39m=\u001b[39mcond,\n\u001b[1;32m    329\u001b[0m         shape\u001b[39m=\u001b[39m[batch_size, c, h \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f, w \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    334\u001b[0m     )\n",
      "File \u001b[0;32m~/.cloned/kcg-ml-sd1p4/stable_diffusion/stable_diffusion.py:175\u001b[0m, in \u001b[0;36mStableDiffusion.get_text_conditioning\u001b[0;34m(self, uncond_scale, prompts, batch_size)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_conditioning\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39m, uncond_scale: \u001b[39mfloat\u001b[39m, prompts: \u001b[39mlist\u001b[39m, batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    172\u001b[0m ):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m uncond_scale \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m--> 175\u001b[0m         un_cond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mget_text_conditioning(batch_size \u001b[39m*\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         un_cond \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cloned/kcg-ml-sd1p4/stable_diffusion/latent_diffusion.py:245\u001b[0m, in \u001b[0;36mLatentDiffusion.get_text_conditioning\u001b[0;34m(self, prompts)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_conditioning\u001b[39m(\u001b[39mself\u001b[39m, prompts: List[\u001b[39mstr\u001b[39m]):\n\u001b[1;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m    ### Get [CLIP embeddings](model/clip_embedder.html) for a list of text prompts\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcond_stage_model(prompts)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "stable_diffusion.generate_images(prompt=\"A cat digging a hole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stable_diffusion(device, pt, sampler_name=\"ddim\", n_steps=20, ddim_eta=0.0):\n",
    "    device = get_device(device)\n",
    "\n",
    "    stable_diffusion = StableDiffusion(\n",
    "        device=device, sampler_name=sampler_name, n_steps=n_steps, ddim_eta=ddim_eta\n",
    "    )\n",
    "\n",
    "    stable_diffusion.quick_initialize()\n",
    "    stable_diffusion.model.load_unet(**pt.unet)\n",
    "    stable_diffusion.model.load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "\n",
    "    return stable_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "Autoencoder loaded from: c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\autoencoder.ckpt\n",
      "Decoder loaded from: c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\decoder.ckpt\n"
     ]
    }
   ],
   "source": [
    "sd = init_stable_diffusion(\"cuda\", pt, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LatentDiffusion:\n\tMissing key(s) in state_dict: \"beta\", \"alpha_bar\". \n\tUnexpected key(s) in state_dict: \"alphas_cumprod\", \"alphas_cumprod_prev\", \"betas\", \"cond_stage_model.transformer.text_model.embeddings.position_embedding.weight\", \"cond_stage_model.transformer.text_model.embeddings.position_ids\", \"cond_stage_model.transformer.text_model.embeddings.token_embedding.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.final_layer_norm.bias\", \"cond_stage_model.transformer.text_model.final_layer_norm.weight\", \"log_one_minus_alphas_cumprod\", \"model_ema.decay\", \"model_ema.num_updates\", \"posterior_log_variance_clipped\", \"posterior_mean_coef1\", \"posterior_mean_coef2\", \"posterior_variance\", \"sqrt_alphas_cumprod\", \"sqrt_one_minus_alphas_cumprod\", \"sqrt_recip_alphas_cumprod\", \"sqrt_recipm1_alphas_cumprod\", \"first_stage_model.encoder.conv_in.bias\", \"first_stage_model.encoder.conv_in.weight\", \"first_stage_model.encoder.conv_out.bias\", \"first_stage_model.encoder.conv_out.weight\", \"first_stage_model.encoder.down.0.block.0.conv1.bias\", \"first_stage_model.encoder.down.0.block.0.conv1.weight\", \"first_stage_model.encoder.down.0.block.0.conv2.bias\", \"first_stage_model.encoder.down.0.block.0.conv2.weight\", \"first_stage_model.encoder.down.0.block.0.norm1.bias\", \"first_stage_model.encoder.down.0.block.0.norm1.weight\", \"first_stage_model.encoder.down.0.block.0.norm2.bias\", \"first_stage_model.encoder.down.0.block.0.norm2.weight\", \"first_stage_model.encoder.down.0.block.1.conv1.bias\", \"first_stage_model.encoder.down.0.block.1.conv1.weight\", \"first_stage_model.encoder.down.0.block.1.conv2.bias\", \"first_stage_model.encoder.down.0.block.1.conv2.weight\", \"first_stage_model.encoder.down.0.block.1.norm1.bias\", \"first_stage_model.encoder.down.0.block.1.norm1.weight\", \"first_stage_model.encoder.down.0.block.1.norm2.bias\", \"first_stage_model.encoder.down.0.block.1.norm2.weight\", \"first_stage_model.encoder.down.0.downsample.conv.bias\", \"first_stage_model.encoder.down.0.downsample.conv.weight\", \"first_stage_model.encoder.down.1.block.0.conv1.bias\", \"first_stage_model.encoder.down.1.block.0.conv1.weight\", \"first_stage_model.encoder.down.1.block.0.conv2.bias\", \"first_stage_model.encoder.down.1.block.0.conv2.weight\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.1.block.0.norm1.bias\", \"first_stage_model.encoder.down.1.block.0.norm1.weight\", \"first_stage_model.encoder.down.1.block.0.norm2.bias\", \"first_stage_model.encoder.down.1.block.0.norm2.weight\", \"first_stage_model.encoder.down.1.block.1.conv1.bias\", \"first_stage_model.encoder.down.1.block.1.conv1.weight\", \"first_stage_model.encoder.down.1.block.1.conv2.bias\", \"first_stage_model.encoder.down.1.block.1.conv2.weight\", \"first_stage_model.encoder.down.1.block.1.norm1.bias\", \"first_stage_model.encoder.down.1.block.1.norm1.weight\", \"first_stage_model.encoder.down.1.block.1.norm2.bias\", \"first_stage_model.encoder.down.1.block.1.norm2.weight\", \"first_stage_model.encoder.down.1.downsample.conv.bias\", \"first_stage_model.encoder.down.1.downsample.conv.weight\", \"first_stage_model.encoder.down.2.block.0.conv1.bias\", \"first_stage_model.encoder.down.2.block.0.conv1.weight\", \"first_stage_model.encoder.down.2.block.0.conv2.bias\", \"first_stage_model.encoder.down.2.block.0.conv2.weight\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.2.block.0.norm1.bias\", \"first_stage_model.encoder.down.2.block.0.norm1.weight\", \"first_stage_model.encoder.down.2.block.0.norm2.bias\", \"first_stage_model.encoder.down.2.block.0.norm2.weight\", \"first_stage_model.encoder.down.2.block.1.conv1.bias\", \"first_stage_model.encoder.down.2.block.1.conv1.weight\", \"first_stage_model.encoder.down.2.block.1.conv2.bias\", \"first_stage_model.encoder.down.2.block.1.conv2.weight\", \"first_stage_model.encoder.down.2.block.1.norm1.bias\", \"first_stage_model.encoder.down.2.block.1.norm1.weight\", \"first_stage_model.encoder.down.2.block.1.norm2.bias\", \"first_stage_model.encoder.down.2.block.1.norm2.weight\", \"first_stage_model.encoder.down.2.downsample.conv.bias\", \"first_stage_model.encoder.down.2.downsample.conv.weight\", \"first_stage_model.encoder.down.3.block.0.conv1.bias\", \"first_stage_model.encoder.down.3.block.0.conv1.weight\", \"first_stage_model.encoder.down.3.block.0.conv2.bias\", \"first_stage_model.encoder.down.3.block.0.conv2.weight\", \"first_stage_model.encoder.down.3.block.0.norm1.bias\", \"first_stage_model.encoder.down.3.block.0.norm1.weight\", \"first_stage_model.encoder.down.3.block.0.norm2.bias\", \"first_stage_model.encoder.down.3.block.0.norm2.weight\", \"first_stage_model.encoder.down.3.block.1.conv1.bias\", \"first_stage_model.encoder.down.3.block.1.conv1.weight\", \"first_stage_model.encoder.down.3.block.1.conv2.bias\", \"first_stage_model.encoder.down.3.block.1.conv2.weight\", \"first_stage_model.encoder.down.3.block.1.norm1.bias\", \"first_stage_model.encoder.down.3.block.1.norm1.weight\", \"first_stage_model.encoder.down.3.block.1.norm2.bias\", \"first_stage_model.encoder.down.3.block.1.norm2.weight\", \"first_stage_model.encoder.mid.attn_1.k.bias\", \"first_stage_model.encoder.mid.attn_1.k.weight\", \"first_stage_model.encoder.mid.attn_1.norm.bias\", \"first_stage_model.encoder.mid.attn_1.norm.weight\", \"first_stage_model.encoder.mid.attn_1.proj_out.bias\", \"first_stage_model.encoder.mid.attn_1.proj_out.weight\", \"first_stage_model.encoder.mid.attn_1.q.bias\", \"first_stage_model.encoder.mid.attn_1.q.weight\", \"first_stage_model.encoder.mid.attn_1.v.bias\", \"first_stage_model.encoder.mid.attn_1.v.weight\", \"first_stage_model.encoder.mid.block_1.conv1.bias\", \"first_stage_model.encoder.mid.block_1.conv1.weight\", \"first_stage_model.encoder.mid.block_1.conv2.bias\", \"first_stage_model.encoder.mid.block_1.conv2.weight\", \"first_stage_model.encoder.mid.block_1.norm1.bias\", \"first_stage_model.encoder.mid.block_1.norm1.weight\", \"first_stage_model.encoder.mid.block_1.norm2.bias\", \"first_stage_model.encoder.mid.block_1.norm2.weight\", \"first_stage_model.encoder.mid.block_2.conv1.bias\", \"first_stage_model.encoder.mid.block_2.conv1.weight\", \"first_stage_model.encoder.mid.block_2.conv2.bias\", \"first_stage_model.encoder.mid.block_2.conv2.weight\", \"first_stage_model.encoder.mid.block_2.norm1.bias\", \"first_stage_model.encoder.mid.block_2.norm1.weight\", \"first_stage_model.encoder.mid.block_2.norm2.bias\", \"first_stage_model.encoder.mid.block_2.norm2.weight\", \"first_stage_model.encoder.norm_out.bias\", \"first_stage_model.encoder.norm_out.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sd\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(tensors)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LatentDiffusion:\n\tMissing key(s) in state_dict: \"beta\", \"alpha_bar\". \n\tUnexpected key(s) in state_dict: \"alphas_cumprod\", \"alphas_cumprod_prev\", \"betas\", \"cond_stage_model.transformer.text_model.embeddings.position_embedding.weight\", \"cond_stage_model.transformer.text_model.embeddings.position_ids\", \"cond_stage_model.transformer.text_model.embeddings.token_embedding.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.final_layer_norm.bias\", \"cond_stage_model.transformer.text_model.final_layer_norm.weight\", \"log_one_minus_alphas_cumprod\", \"model_ema.decay\", \"model_ema.num_updates\", \"posterior_log_variance_clipped\", \"posterior_mean_coef1\", \"posterior_mean_coef2\", \"posterior_variance\", \"sqrt_alphas_cumprod\", \"sqrt_one_minus_alphas_cumprod\", \"sqrt_recip_alphas_cumprod\", \"sqrt_recipm1_alphas_cumprod\", \"first_stage_model.encoder.conv_in.bias\", \"first_stage_model.encoder.conv_in.weight\", \"first_stage_model.encoder.conv_out.bias\", \"first_stage_model.encoder.conv_out.weight\", \"first_stage_model.encoder.down.0.block.0.conv1.bias\", \"first_stage_model.encoder.down.0.block.0.conv1.weight\", \"first_stage_model.encoder.down.0.block.0.conv2.bias\", \"first_stage_model.encoder.down.0.block.0.conv2.weight\", \"first_stage_model.encoder.down.0.block.0.norm1.bias\", \"first_stage_model.encoder.down.0.block.0.norm1.weight\", \"first_stage_model.encoder.down.0.block.0.norm2.bias\", \"first_stage_model.encoder.down.0.block.0.norm2.weight\", \"first_stage_model.encoder.down.0.block.1.conv1.bias\", \"first_stage_model.encoder.down.0.block.1.conv1.weight\", \"first_stage_model.encoder.down.0.block.1.conv2.bias\", \"first_stage_model.encoder.down.0.block.1.conv2.weight\", \"first_stage_model.encoder.down.0.block.1.norm1.bias\", \"first_stage_model.encoder.down.0.block.1.norm1.weight\", \"first_stage_model.encoder.down.0.block.1.norm2.bias\", \"first_stage_model.encoder.down.0.block.1.norm2.weight\", \"first_stage_model.encoder.down.0.downsample.conv.bias\", \"first_stage_model.encoder.down.0.downsample.conv.weight\", \"first_stage_model.encoder.down.1.block.0.conv1.bias\", \"first_stage_model.encoder.down.1.block.0.conv1.weight\", \"first_stage_model.encoder.down.1.block.0.conv2.bias\", \"first_stage_model.encoder.down.1.block.0.conv2.weight\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.1.block.0.norm1.bias\", \"first_stage_model.encoder.down.1.block.0.norm1.weight\", \"first_stage_model.encoder.down.1.block.0.norm2.bias\", \"first_stage_model.encoder.down.1.block.0.norm2.weight\", \"first_stage_model.encoder.down.1.block.1.conv1.bias\", \"first_stage_model.encoder.down.1.block.1.conv1.weight\", \"first_stage_model.encoder.down.1.block.1.conv2.bias\", \"first_stage_model.encoder.down.1.block.1.conv2.weight\", \"first_stage_model.encoder.down.1.block.1.norm1.bias\", \"first_stage_model.encoder.down.1.block.1.norm1.weight\", \"first_stage_model.encoder.down.1.block.1.norm2.bias\", \"first_stage_model.encoder.down.1.block.1.norm2.weight\", \"first_stage_model.encoder.down.1.downsample.conv.bias\", \"first_stage_model.encoder.down.1.downsample.conv.weight\", \"first_stage_model.encoder.down.2.block.0.conv1.bias\", \"first_stage_model.encoder.down.2.block.0.conv1.weight\", \"first_stage_model.encoder.down.2.block.0.conv2.bias\", \"first_stage_model.encoder.down.2.block.0.conv2.weight\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.2.block.0.norm1.bias\", \"first_stage_model.encoder.down.2.block.0.norm1.weight\", \"first_stage_model.encoder.down.2.block.0.norm2.bias\", \"first_stage_model.encoder.down.2.block.0.norm2.weight\", \"first_stage_model.encoder.down.2.block.1.conv1.bias\", \"first_stage_model.encoder.down.2.block.1.conv1.weight\", \"first_stage_model.encoder.down.2.block.1.conv2.bias\", \"first_stage_model.encoder.down.2.block.1.conv2.weight\", \"first_stage_model.encoder.down.2.block.1.norm1.bias\", \"first_stage_model.encoder.down.2.block.1.norm1.weight\", \"first_stage_model.encoder.down.2.block.1.norm2.bias\", \"first_stage_model.encoder.down.2.block.1.norm2.weight\", \"first_stage_model.encoder.down.2.downsample.conv.bias\", \"first_stage_model.encoder.down.2.downsample.conv.weight\", \"first_stage_model.encoder.down.3.block.0.conv1.bias\", \"first_stage_model.encoder.down.3.block.0.conv1.weight\", \"first_stage_model.encoder.down.3.block.0.conv2.bias\", \"first_stage_model.encoder.down.3.block.0.conv2.weight\", \"first_stage_model.encoder.down.3.block.0.norm1.bias\", \"first_stage_model.encoder.down.3.block.0.norm1.weight\", \"first_stage_model.encoder.down.3.block.0.norm2.bias\", \"first_stage_model.encoder.down.3.block.0.norm2.weight\", \"first_stage_model.encoder.down.3.block.1.conv1.bias\", \"first_stage_model.encoder.down.3.block.1.conv1.weight\", \"first_stage_model.encoder.down.3.block.1.conv2.bias\", \"first_stage_model.encoder.down.3.block.1.conv2.weight\", \"first_stage_model.encoder.down.3.block.1.norm1.bias\", \"first_stage_model.encoder.down.3.block.1.norm1.weight\", \"first_stage_model.encoder.down.3.block.1.norm2.bias\", \"first_stage_model.encoder.down.3.block.1.norm2.weight\", \"first_stage_model.encoder.mid.attn_1.k.bias\", \"first_stage_model.encoder.mid.attn_1.k.weight\", \"first_stage_model.encoder.mid.attn_1.norm.bias\", \"first_stage_model.encoder.mid.attn_1.norm.weight\", \"first_stage_model.encoder.mid.attn_1.proj_out.bias\", \"first_stage_model.encoder.mid.attn_1.proj_out.weight\", \"first_stage_model.encoder.mid.attn_1.q.bias\", \"first_stage_model.encoder.mid.attn_1.q.weight\", \"first_stage_model.encoder.mid.attn_1.v.bias\", \"first_stage_model.encoder.mid.attn_1.v.weight\", \"first_stage_model.encoder.mid.block_1.conv1.bias\", \"first_stage_model.encoder.mid.block_1.conv1.weight\", \"first_stage_model.encoder.mid.block_1.conv2.bias\", \"first_stage_model.encoder.mid.block_1.conv2.weight\", \"first_stage_model.encoder.mid.block_1.norm1.bias\", \"first_stage_model.encoder.mid.block_1.norm1.weight\", \"first_stage_model.encoder.mid.block_1.norm2.bias\", \"first_stage_model.encoder.mid.block_1.norm2.weight\", \"first_stage_model.encoder.mid.block_2.conv1.bias\", \"first_stage_model.encoder.mid.block_2.conv1.weight\", \"first_stage_model.encoder.mid.block_2.conv2.bias\", \"first_stage_model.encoder.mid.block_2.conv2.weight\", \"first_stage_model.encoder.mid.block_2.norm1.bias\", \"first_stage_model.encoder.mid.block_2.norm1.weight\", \"first_stage_model.encoder.mid.block_2.norm2.bias\", \"first_stage_model.encoder.mid.block_2.norm2.weight\", \"first_stage_model.encoder.norm_out.bias\", \"first_stage_model.encoder.norm_out.weight\". "
     ]
    }
   ],
   "source": [
    "sd.model.load_state_dict(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "sd.model.state_dict();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(sd.model.state_dict(), \"v1-5-pruned-emaonly.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a424b5544544faf8b8ae4dcbb918451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\">autoencoder initialization...</pre>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    }
   ],
   "source": [
    "ld = initialize_latent_diffusion(path = pt.checkpoint_path_st, device=\"cuda\", force_submodels_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a dict of [str, torch.Tensor] but received <class 'stable_diffusion.model.vae.autoencoder.Autoencoder'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_file(ld\u001b[39m.\u001b[39;49mfirst_stage_model, \u001b[39m\"\u001b[39;49m\u001b[39mautoencoder.safetensors\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\safetensors\\torch.py:232\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(\n\u001b[0;32m    202\u001b[0m     tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m    203\u001b[0m     filename: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[0;32m    204\u001b[0m     metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    205\u001b[0m ):\n\u001b[0;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[39m=\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\safetensors\\torch.py:379\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBig endian is not supported, serialization need to be in little endian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, \u001b[39mdict\u001b[39m):\n\u001b[1;32m--> 379\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict of [str, torch.Tensor] but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    380\u001b[0m ptrs \u001b[39m=\u001b[39m defaultdict(\u001b[39mset\u001b[39m)\n\u001b[0;32m    381\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a dict of [str, torch.Tensor] but received <class 'stable_diffusion.model.vae.autoencoder.Autoencoder'>"
     ]
    }
   ],
   "source": [
    "save_file(ld.first_stage_model, \"autoencoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_file(\"autoencoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
