{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from safetensors.torch import load_file, load_model\n",
    "from stable_diffusion import StableDiffusion\n",
    "from stable_diffusion.utils.model import *\n",
    "from stable_diffusion.utils.utils import get_device\n",
    "from stable_diffusion.constants import ModelsPathTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = ModelsPathTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device: NVIDIA GeForce RTX 3080 Ti\n",
      "INFO: Device given. Using device cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
      "INFO: Device given. Using device cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDiffusion(\n",
       "  (model): UNetWrapper()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "stable_diffusion = StableDiffusion(\n",
    "        device=device\n",
    "    )\n",
    "stable_diffusion.quick_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(stable_diffusion.model, os.path.abspath(r\"E:\\v1-5-pruned-emaonly.safetensors\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stable_diffusion\u001b[39m.\u001b[39;49mgenerate_images(prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mA cat digging a hole\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion\\stable_diffusion.py:321\u001b[0m, in \u001b[0;36mStableDiffusion.generate_images\u001b[1;34m(self, seed, batch_size, prompt, h, w, uncond_scale, low_vram, noise_fn, temperature)\u001b[0m\n\u001b[0;32m    318\u001b[0m autocast \u001b[39m=\u001b[39m get_autocast()\n\u001b[0;32m    319\u001b[0m \u001b[39mwith\u001b[39;00m autocast:\n\u001b[0;32m    320\u001b[0m     \u001b[39m# with section(\"getting text cond\"):\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     un_cond, cond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_text_conditioning(\n\u001b[0;32m    322\u001b[0m         uncond_scale, prompts, batch_size\n\u001b[0;32m    323\u001b[0m     )\n\u001b[0;32m    324\u001b[0m     \u001b[39m# [Sample in the latent space](../sampler/index.html).\u001b[39;00m\n\u001b[0;32m    325\u001b[0m     \u001b[39m# `x` will be of shape `[batch_size, c, h / f, w / f]`\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[39m# with section(\"sampling\"):\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39msample(\n\u001b[0;32m    328\u001b[0m         cond\u001b[39m=\u001b[39mcond,\n\u001b[0;32m    329\u001b[0m         shape\u001b[39m=\u001b[39m[batch_size, c, h \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f, w \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m f],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m    334\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion\\stable_diffusion.py:175\u001b[0m, in \u001b[0;36mStableDiffusion.get_text_conditioning\u001b[1;34m(self, uncond_scale, prompts, batch_size)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_conditioning\u001b[39m(\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39m, uncond_scale: \u001b[39mfloat\u001b[39m, prompts: \u001b[39mlist\u001b[39m, batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    172\u001b[0m ):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# In unconditional scaling is not $1$ get the embeddings for empty prompts (no conditioning).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m uncond_scale \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m--> 175\u001b[0m         un_cond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mget_text_conditioning(batch_size \u001b[39m*\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m         un_cond \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion\\latent_diffusion.py:245\u001b[0m, in \u001b[0;36mLatentDiffusion.get_text_conditioning\u001b[1;34m(self, prompts)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text_conditioning\u001b[39m(\u001b[39mself\u001b[39m, prompts: List[\u001b[39mstr\u001b[39m]):\n\u001b[0;32m    242\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    ### Get [CLIP embeddings](model/clip_embedder.html) for a list of text prompts\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcond_stage_model(prompts)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "stable_diffusion.generate_images(prompt=\"A cat digging a hole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stable_diffusion(device, pt, sampler_name=\"ddim\", n_steps=20, ddim_eta=0.0):\n",
    "    device = get_device(device)\n",
    "\n",
    "    stable_diffusion = StableDiffusion(\n",
    "        device=device, sampler_name=sampler_name, n_steps=n_steps, ddim_eta=ddim_eta\n",
    "    )\n",
    "\n",
    "    stable_diffusion.quick_initialize()\n",
    "    stable_diffusion.model.load_unet(**pt.unet)\n",
    "    stable_diffusion.model.load_autoencoder(**pt.autoencoder).load_decoder(**pt.decoder)\n",
    "\n",
    "    return stable_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "WARNING: `LatentDiffusion` model is `None` given. Initialize one with the appropriate method.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "Autoencoder loaded from: c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\autoencoder.ckpt\n",
      "Decoder loaded from: c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\decoder.ckpt\n"
     ]
    }
   ],
   "source": [
    "sd = init_stable_diffusion(\"cuda\", pt, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LatentDiffusion:\n\tMissing key(s) in state_dict: \"beta\", \"alpha_bar\". \n\tUnexpected key(s) in state_dict: \"alphas_cumprod\", \"alphas_cumprod_prev\", \"betas\", \"cond_stage_model.transformer.text_model.embeddings.position_embedding.weight\", \"cond_stage_model.transformer.text_model.embeddings.position_ids\", \"cond_stage_model.transformer.text_model.embeddings.token_embedding.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.final_layer_norm.bias\", \"cond_stage_model.transformer.text_model.final_layer_norm.weight\", \"log_one_minus_alphas_cumprod\", \"model_ema.decay\", \"model_ema.num_updates\", \"posterior_log_variance_clipped\", \"posterior_mean_coef1\", \"posterior_mean_coef2\", \"posterior_variance\", \"sqrt_alphas_cumprod\", \"sqrt_one_minus_alphas_cumprod\", \"sqrt_recip_alphas_cumprod\", \"sqrt_recipm1_alphas_cumprod\", \"first_stage_model.encoder.conv_in.bias\", \"first_stage_model.encoder.conv_in.weight\", \"first_stage_model.encoder.conv_out.bias\", \"first_stage_model.encoder.conv_out.weight\", \"first_stage_model.encoder.down.0.block.0.conv1.bias\", \"first_stage_model.encoder.down.0.block.0.conv1.weight\", \"first_stage_model.encoder.down.0.block.0.conv2.bias\", \"first_stage_model.encoder.down.0.block.0.conv2.weight\", \"first_stage_model.encoder.down.0.block.0.norm1.bias\", \"first_stage_model.encoder.down.0.block.0.norm1.weight\", \"first_stage_model.encoder.down.0.block.0.norm2.bias\", \"first_stage_model.encoder.down.0.block.0.norm2.weight\", \"first_stage_model.encoder.down.0.block.1.conv1.bias\", \"first_stage_model.encoder.down.0.block.1.conv1.weight\", \"first_stage_model.encoder.down.0.block.1.conv2.bias\", \"first_stage_model.encoder.down.0.block.1.conv2.weight\", \"first_stage_model.encoder.down.0.block.1.norm1.bias\", \"first_stage_model.encoder.down.0.block.1.norm1.weight\", \"first_stage_model.encoder.down.0.block.1.norm2.bias\", \"first_stage_model.encoder.down.0.block.1.norm2.weight\", \"first_stage_model.encoder.down.0.downsample.conv.bias\", \"first_stage_model.encoder.down.0.downsample.conv.weight\", \"first_stage_model.encoder.down.1.block.0.conv1.bias\", \"first_stage_model.encoder.down.1.block.0.conv1.weight\", \"first_stage_model.encoder.down.1.block.0.conv2.bias\", \"first_stage_model.encoder.down.1.block.0.conv2.weight\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.1.block.0.norm1.bias\", \"first_stage_model.encoder.down.1.block.0.norm1.weight\", \"first_stage_model.encoder.down.1.block.0.norm2.bias\", \"first_stage_model.encoder.down.1.block.0.norm2.weight\", \"first_stage_model.encoder.down.1.block.1.conv1.bias\", \"first_stage_model.encoder.down.1.block.1.conv1.weight\", \"first_stage_model.encoder.down.1.block.1.conv2.bias\", \"first_stage_model.encoder.down.1.block.1.conv2.weight\", \"first_stage_model.encoder.down.1.block.1.norm1.bias\", \"first_stage_model.encoder.down.1.block.1.norm1.weight\", \"first_stage_model.encoder.down.1.block.1.norm2.bias\", \"first_stage_model.encoder.down.1.block.1.norm2.weight\", \"first_stage_model.encoder.down.1.downsample.conv.bias\", \"first_stage_model.encoder.down.1.downsample.conv.weight\", \"first_stage_model.encoder.down.2.block.0.conv1.bias\", \"first_stage_model.encoder.down.2.block.0.conv1.weight\", \"first_stage_model.encoder.down.2.block.0.conv2.bias\", \"first_stage_model.encoder.down.2.block.0.conv2.weight\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.2.block.0.norm1.bias\", \"first_stage_model.encoder.down.2.block.0.norm1.weight\", \"first_stage_model.encoder.down.2.block.0.norm2.bias\", \"first_stage_model.encoder.down.2.block.0.norm2.weight\", \"first_stage_model.encoder.down.2.block.1.conv1.bias\", \"first_stage_model.encoder.down.2.block.1.conv1.weight\", \"first_stage_model.encoder.down.2.block.1.conv2.bias\", \"first_stage_model.encoder.down.2.block.1.conv2.weight\", \"first_stage_model.encoder.down.2.block.1.norm1.bias\", \"first_stage_model.encoder.down.2.block.1.norm1.weight\", \"first_stage_model.encoder.down.2.block.1.norm2.bias\", \"first_stage_model.encoder.down.2.block.1.norm2.weight\", \"first_stage_model.encoder.down.2.downsample.conv.bias\", \"first_stage_model.encoder.down.2.downsample.conv.weight\", \"first_stage_model.encoder.down.3.block.0.conv1.bias\", \"first_stage_model.encoder.down.3.block.0.conv1.weight\", \"first_stage_model.encoder.down.3.block.0.conv2.bias\", \"first_stage_model.encoder.down.3.block.0.conv2.weight\", \"first_stage_model.encoder.down.3.block.0.norm1.bias\", \"first_stage_model.encoder.down.3.block.0.norm1.weight\", \"first_stage_model.encoder.down.3.block.0.norm2.bias\", \"first_stage_model.encoder.down.3.block.0.norm2.weight\", \"first_stage_model.encoder.down.3.block.1.conv1.bias\", \"first_stage_model.encoder.down.3.block.1.conv1.weight\", \"first_stage_model.encoder.down.3.block.1.conv2.bias\", \"first_stage_model.encoder.down.3.block.1.conv2.weight\", \"first_stage_model.encoder.down.3.block.1.norm1.bias\", \"first_stage_model.encoder.down.3.block.1.norm1.weight\", \"first_stage_model.encoder.down.3.block.1.norm2.bias\", \"first_stage_model.encoder.down.3.block.1.norm2.weight\", \"first_stage_model.encoder.mid.attn_1.k.bias\", \"first_stage_model.encoder.mid.attn_1.k.weight\", \"first_stage_model.encoder.mid.attn_1.norm.bias\", \"first_stage_model.encoder.mid.attn_1.norm.weight\", \"first_stage_model.encoder.mid.attn_1.proj_out.bias\", \"first_stage_model.encoder.mid.attn_1.proj_out.weight\", \"first_stage_model.encoder.mid.attn_1.q.bias\", \"first_stage_model.encoder.mid.attn_1.q.weight\", \"first_stage_model.encoder.mid.attn_1.v.bias\", \"first_stage_model.encoder.mid.attn_1.v.weight\", \"first_stage_model.encoder.mid.block_1.conv1.bias\", \"first_stage_model.encoder.mid.block_1.conv1.weight\", \"first_stage_model.encoder.mid.block_1.conv2.bias\", \"first_stage_model.encoder.mid.block_1.conv2.weight\", \"first_stage_model.encoder.mid.block_1.norm1.bias\", \"first_stage_model.encoder.mid.block_1.norm1.weight\", \"first_stage_model.encoder.mid.block_1.norm2.bias\", \"first_stage_model.encoder.mid.block_1.norm2.weight\", \"first_stage_model.encoder.mid.block_2.conv1.bias\", \"first_stage_model.encoder.mid.block_2.conv1.weight\", \"first_stage_model.encoder.mid.block_2.conv2.bias\", \"first_stage_model.encoder.mid.block_2.conv2.weight\", \"first_stage_model.encoder.mid.block_2.norm1.bias\", \"first_stage_model.encoder.mid.block_2.norm1.weight\", \"first_stage_model.encoder.mid.block_2.norm2.bias\", \"first_stage_model.encoder.mid.block_2.norm2.weight\", \"first_stage_model.encoder.norm_out.bias\", \"first_stage_model.encoder.norm_out.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sd\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_state_dict(tensors)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LatentDiffusion:\n\tMissing key(s) in state_dict: \"beta\", \"alpha_bar\". \n\tUnexpected key(s) in state_dict: \"alphas_cumprod\", \"alphas_cumprod_prev\", \"betas\", \"cond_stage_model.transformer.text_model.embeddings.position_embedding.weight\", \"cond_stage_model.transformer.text_model.embeddings.position_ids\", \"cond_stage_model.transformer.text_model.embeddings.token_embedding.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias\", \"cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight\", \"cond_stage_model.transformer.text_model.final_layer_norm.bias\", \"cond_stage_model.transformer.text_model.final_layer_norm.weight\", \"log_one_minus_alphas_cumprod\", \"model_ema.decay\", \"model_ema.num_updates\", \"posterior_log_variance_clipped\", \"posterior_mean_coef1\", \"posterior_mean_coef2\", \"posterior_variance\", \"sqrt_alphas_cumprod\", \"sqrt_one_minus_alphas_cumprod\", \"sqrt_recip_alphas_cumprod\", \"sqrt_recipm1_alphas_cumprod\", \"first_stage_model.encoder.conv_in.bias\", \"first_stage_model.encoder.conv_in.weight\", \"first_stage_model.encoder.conv_out.bias\", \"first_stage_model.encoder.conv_out.weight\", \"first_stage_model.encoder.down.0.block.0.conv1.bias\", \"first_stage_model.encoder.down.0.block.0.conv1.weight\", \"first_stage_model.encoder.down.0.block.0.conv2.bias\", \"first_stage_model.encoder.down.0.block.0.conv2.weight\", \"first_stage_model.encoder.down.0.block.0.norm1.bias\", \"first_stage_model.encoder.down.0.block.0.norm1.weight\", \"first_stage_model.encoder.down.0.block.0.norm2.bias\", \"first_stage_model.encoder.down.0.block.0.norm2.weight\", \"first_stage_model.encoder.down.0.block.1.conv1.bias\", \"first_stage_model.encoder.down.0.block.1.conv1.weight\", \"first_stage_model.encoder.down.0.block.1.conv2.bias\", \"first_stage_model.encoder.down.0.block.1.conv2.weight\", \"first_stage_model.encoder.down.0.block.1.norm1.bias\", \"first_stage_model.encoder.down.0.block.1.norm1.weight\", \"first_stage_model.encoder.down.0.block.1.norm2.bias\", \"first_stage_model.encoder.down.0.block.1.norm2.weight\", \"first_stage_model.encoder.down.0.downsample.conv.bias\", \"first_stage_model.encoder.down.0.downsample.conv.weight\", \"first_stage_model.encoder.down.1.block.0.conv1.bias\", \"first_stage_model.encoder.down.1.block.0.conv1.weight\", \"first_stage_model.encoder.down.1.block.0.conv2.bias\", \"first_stage_model.encoder.down.1.block.0.conv2.weight\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.1.block.0.norm1.bias\", \"first_stage_model.encoder.down.1.block.0.norm1.weight\", \"first_stage_model.encoder.down.1.block.0.norm2.bias\", \"first_stage_model.encoder.down.1.block.0.norm2.weight\", \"first_stage_model.encoder.down.1.block.1.conv1.bias\", \"first_stage_model.encoder.down.1.block.1.conv1.weight\", \"first_stage_model.encoder.down.1.block.1.conv2.bias\", \"first_stage_model.encoder.down.1.block.1.conv2.weight\", \"first_stage_model.encoder.down.1.block.1.norm1.bias\", \"first_stage_model.encoder.down.1.block.1.norm1.weight\", \"first_stage_model.encoder.down.1.block.1.norm2.bias\", \"first_stage_model.encoder.down.1.block.1.norm2.weight\", \"first_stage_model.encoder.down.1.downsample.conv.bias\", \"first_stage_model.encoder.down.1.downsample.conv.weight\", \"first_stage_model.encoder.down.2.block.0.conv1.bias\", \"first_stage_model.encoder.down.2.block.0.conv1.weight\", \"first_stage_model.encoder.down.2.block.0.conv2.bias\", \"first_stage_model.encoder.down.2.block.0.conv2.weight\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\", \"first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\", \"first_stage_model.encoder.down.2.block.0.norm1.bias\", \"first_stage_model.encoder.down.2.block.0.norm1.weight\", \"first_stage_model.encoder.down.2.block.0.norm2.bias\", \"first_stage_model.encoder.down.2.block.0.norm2.weight\", \"first_stage_model.encoder.down.2.block.1.conv1.bias\", \"first_stage_model.encoder.down.2.block.1.conv1.weight\", \"first_stage_model.encoder.down.2.block.1.conv2.bias\", \"first_stage_model.encoder.down.2.block.1.conv2.weight\", \"first_stage_model.encoder.down.2.block.1.norm1.bias\", \"first_stage_model.encoder.down.2.block.1.norm1.weight\", \"first_stage_model.encoder.down.2.block.1.norm2.bias\", \"first_stage_model.encoder.down.2.block.1.norm2.weight\", \"first_stage_model.encoder.down.2.downsample.conv.bias\", \"first_stage_model.encoder.down.2.downsample.conv.weight\", \"first_stage_model.encoder.down.3.block.0.conv1.bias\", \"first_stage_model.encoder.down.3.block.0.conv1.weight\", \"first_stage_model.encoder.down.3.block.0.conv2.bias\", \"first_stage_model.encoder.down.3.block.0.conv2.weight\", \"first_stage_model.encoder.down.3.block.0.norm1.bias\", \"first_stage_model.encoder.down.3.block.0.norm1.weight\", \"first_stage_model.encoder.down.3.block.0.norm2.bias\", \"first_stage_model.encoder.down.3.block.0.norm2.weight\", \"first_stage_model.encoder.down.3.block.1.conv1.bias\", \"first_stage_model.encoder.down.3.block.1.conv1.weight\", \"first_stage_model.encoder.down.3.block.1.conv2.bias\", \"first_stage_model.encoder.down.3.block.1.conv2.weight\", \"first_stage_model.encoder.down.3.block.1.norm1.bias\", \"first_stage_model.encoder.down.3.block.1.norm1.weight\", \"first_stage_model.encoder.down.3.block.1.norm2.bias\", \"first_stage_model.encoder.down.3.block.1.norm2.weight\", \"first_stage_model.encoder.mid.attn_1.k.bias\", \"first_stage_model.encoder.mid.attn_1.k.weight\", \"first_stage_model.encoder.mid.attn_1.norm.bias\", \"first_stage_model.encoder.mid.attn_1.norm.weight\", \"first_stage_model.encoder.mid.attn_1.proj_out.bias\", \"first_stage_model.encoder.mid.attn_1.proj_out.weight\", \"first_stage_model.encoder.mid.attn_1.q.bias\", \"first_stage_model.encoder.mid.attn_1.q.weight\", \"first_stage_model.encoder.mid.attn_1.v.bias\", \"first_stage_model.encoder.mid.attn_1.v.weight\", \"first_stage_model.encoder.mid.block_1.conv1.bias\", \"first_stage_model.encoder.mid.block_1.conv1.weight\", \"first_stage_model.encoder.mid.block_1.conv2.bias\", \"first_stage_model.encoder.mid.block_1.conv2.weight\", \"first_stage_model.encoder.mid.block_1.norm1.bias\", \"first_stage_model.encoder.mid.block_1.norm1.weight\", \"first_stage_model.encoder.mid.block_1.norm2.bias\", \"first_stage_model.encoder.mid.block_1.norm2.weight\", \"first_stage_model.encoder.mid.block_2.conv1.bias\", \"first_stage_model.encoder.mid.block_2.conv1.weight\", \"first_stage_model.encoder.mid.block_2.conv2.bias\", \"first_stage_model.encoder.mid.block_2.conv2.weight\", \"first_stage_model.encoder.mid.block_2.norm1.bias\", \"first_stage_model.encoder.mid.block_2.norm1.weight\", \"first_stage_model.encoder.mid.block_2.norm2.bias\", \"first_stage_model.encoder.mid.block_2.norm2.weight\", \"first_stage_model.encoder.norm_out.bias\", \"first_stage_model.encoder.norm_out.weight\". "
     ]
    }
   ],
   "source": [
    "sd.model.load_state_dict(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "sd.model.state_dict();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(sd.model.state_dict(), \"v1-5-pruned-emaonly.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a424b5544544faf8b8ae4dcbb918451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\">autoencoder initialization...</pre>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'logit_scale', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'text_projection.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Device given. Using device cuda.\n",
      "INFO: Using CUDA device None: NVIDIA GeForce RTX 3080 Ti.\n",
      "INFO: `device` is None. Using device  cuda:0.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    }
   ],
   "source": [
    "ld = initialize_latent_diffusion(path = pt.checkpoint_path_st, device=\"cuda\", force_submodels_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a dict of [str, torch.Tensor] but received <class 'stable_diffusion.model.vae.autoencoder.Autoencoder'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_file(ld\u001b[39m.\u001b[39;49mfirst_stage_model, \u001b[39m\"\u001b[39;49m\u001b[39mautoencoder.safetensors\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\safetensors\\torch.py:232\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(\n\u001b[0;32m    202\u001b[0m     tensors: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m    203\u001b[0m     filename: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[0;32m    204\u001b[0m     metadata: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    205\u001b[0m ):\n\u001b[0;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39m    ```\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[39m=\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\safetensors\\torch.py:379\u001b[0m, in \u001b[0;36m_flatten\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBig endian is not supported, serialization need to be in little endian\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, \u001b[39mdict\u001b[39m):\n\u001b[1;32m--> 379\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a dict of [str, torch.Tensor] but received \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensors)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    380\u001b[0m ptrs \u001b[39m=\u001b[39m defaultdict(\u001b[39mset\u001b[39m)\n\u001b[0;32m    381\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tensors\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a dict of [str, torch.Tensor] but received <class 'stable_diffusion.model.vae.autoencoder.Autoencoder'>"
     ]
    }
   ],
   "source": [
    "save_file(ld.first_stage_model, \"autoencoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = load_file(\"autoencoder.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
