{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "output_directory = \"./output/noise-tests/sd2-notebook\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "from stable_diffusion2.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion2.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion2.utils.model import *\n",
    "from stable_diffusion2.utils.utils import SectionManager as section\n",
    "from stable_diffusion2.utils.utils import *\n",
    "from stable_diffusion2.model.clip.clip_embedder import CLIPTextEmbedder\n",
    "\n",
    "\n",
    "\n",
    "from stable_diffusion2.model.unet.unet import UNetModel\n",
    "\n",
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_distribution(loc, scale):\n",
    "    base_distribution = torch.distributions.Uniform(0, 1)\n",
    "    transforms = [torch.distributions.transforms.SigmoidTransform().inv, torch.distributions.transforms.AffineTransform(loc=loc, scale=scale)]\n",
    "    logistic = torch.distributions.TransformedDistribution(base_distribution, transforms)\n",
    "    return logistic\n",
    "noise_fn = lambda shape, device = device: logistic_distribution(loc=0.0, scale=0.5).sample(shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "WARNING: LatentDiffusion model not given.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Total: 12287 MiB\n",
      "Free: 11084 MiB\n",
      "Used: 1203 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model = StableDiffusion(device=device)\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting section: load submodel tree...\n",
      "Finished section: load submodel tree in 2.73 seconds\n",
      "\n",
      "Total: 12287 MiB\n",
      "Free: 6932 MiB\n",
      "Used: 5355 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.model.load_submodel_tree()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 11084 MiB\n",
      "Used: 1203 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.unload_model()\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "WARNING: LatentDiffusion model not given.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Starting section: load submodel tree...\n",
      "Finished section: load submodel tree in 2.48 seconds\n",
      "\n",
      "Total: 12287 MiB\n",
      "Free: 6932 MiB\n",
      "Used: 5355 MiB\n"
     ]
    }
   ],
   "source": [
    "ddim_eta = 0.00000\n",
    "stable_diffusion_model = StableDiffusion(device = device, ddim_eta=ddim_eta)\n",
    "stable_diffusion_model.model.load_submodel_tree()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting section: getting text cond...\n",
      "Finished section: getting text cond in 0.90 seconds\n",
      "\n",
      "Starting section: sampling...\n",
      "Finished section: sampling in 5.49 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0\n",
    "imgs = stable_diffusion_model.generate_images(\n",
    "    prompt = 'A woman with flowers in her hair in a courtyard, in the style of Frank Frazetta',\n",
    "    seed = 2982,\n",
    "    noise_fn = noise_fn,\n",
    "    temperature = temperature,\n",
    "    )\n",
    "\n",
    "save_images(imgs, join(output_directory, f'test_sample.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9986 MiB\n",
      "Used: 2301 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.unload_model()\n",
    "del imgs\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9986 MiB\n",
      "Used: 2301 MiB\n"
     ]
    }
   ],
   "source": [
    "img = load_img(\"./input/test_img.jpg\").to(device)\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9986 MiB\n",
      "Used: 2301 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.model.load_autoencoder()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9846 MiB\n",
      "Used: 2441 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.model.autoencoder.load_encoder()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9136 MiB\n",
      "Used: 3151 MiB\n"
     ]
    }
   ],
   "source": [
    "encoded_img = stable_diffusion_model.encode(img)\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(encoded_img[:, :3,:,:], join(output_directory, f'test_encoding_1.png'))\n",
    "save_images(encoded_img[:, 1:,:,:], join(output_directory, f'test_encoding_2.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_img, join(output_directory, f'encoded_img_temp{temperature}_eta{ddim_eta:.3f}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9760 MiB\n",
      "Used: 2527 MiB\n",
      "(tensor(9760), tensor(12287))\n"
     ]
    }
   ],
   "source": [
    "del encoded_img\n",
    "torch.cuda.empty_cache()\n",
    "print(get_memory_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9760 MiB\n",
      "Used: 2527 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_img = torch.load(join(output_directory, f'encoded_img_temp{temperature}_eta{ddim_eta:.3f}.pt'))\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status()\n",
    "encoded_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9760 MiB\n",
      "Used: 2527 MiB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9726 MiB\n",
      "Used: 2561 MiB\n"
     ]
    }
   ],
   "source": [
    "stable_diffusion_model.model.autoencoder.load_decoder()\n",
    "torch.cuda.empty_cache()\n",
    "get_memory_status();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 12287 MiB\n",
      "Free: 9746 MiB\n",
      "Used: 2541 MiB\n",
      "(tensor(9746), tensor(12287))\n"
     ]
    }
   ],
   "source": [
    "decoded_img = stable_diffusion_model.decode(encoded_img)\n",
    "save_images(decoded_img, join(output_directory, f'decoded_img_temp{temperature}_eta{ddim_eta:.3f}.png'))\n",
    "torch.cuda.empty_cache()\n",
    "print(get_memory_status())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
