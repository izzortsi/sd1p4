{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using CUDA device: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "from stable_diffusion2.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion2.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion2.utils.model import *\n",
    "from stable_diffusion2.utils.utils import SectionManager as section\n",
    "from stable_diffusion2.utils.utils import *\n",
    "from stable_diffusion2.model.clip.clip_embedder import CLIPTextEmbedder\n",
    "\n",
    "\n",
    "\n",
    "from stable_diffusion2.model.unet.unet import UNetModel\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 11086 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1201 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(11086), tensor(12287))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Falling back to current device.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n"
     ]
    }
   ],
   "source": [
    "latent_diffusion_model = LatentDiffusion(linear_start=0.00085,\n",
    "            linear_end=0.0120,\n",
    "            n_steps=1000,\n",
    "            latent_scaling_factor=0.18215\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 11084 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1203 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(11084), tensor(12287))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_diffusion_model.load_autoencoder()\n",
    "# latent_diffusion_model.load_submodel_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 11084 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1203 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(11084), tensor(12287))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_diffusion_model.first_stage_model.load_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 10938 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1349 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(10938), tensor(12287))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(\"./scripts2/test_img.jpg\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_img = latent_diffusion_model.autoencoder_encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 5720 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 6567 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(5720), tensor(12287))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_diffusion_model.first_stage_model.unload_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 5720 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 6567 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(5720), tensor(12287))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_diffusion_model.first_stage_model.load_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 5592 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 6695 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(5592), tensor(12287))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 12.00 GiB total capacity; 10.95 GiB already allocated; 0 bytes free; 11.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decoded_img \u001b[39m=\u001b[39m latent_diffusion_model\u001b[39m.\u001b[39;49mautoencoder_decode(encoded_img)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion2\\latent_diffusion.py:200\u001b[0m, in \u001b[0;36mLatentDiffusion.autoencoder_decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mautoencoder_decode\u001b[39m(\u001b[39mself\u001b[39m, z: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    195\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m    ### Get image from the latent representation\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m \u001b[39m    We scale down by the scaling factor and then decode.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_stage_model\u001b[39m.\u001b[39;49mdecode(z \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatent_scaling_factor)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion2\\model\\vae\\autoencoder.py:159\u001b[0m, in \u001b[0;36mAutoencoder.decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    157\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m    158\u001b[0m \u001b[39m# Decode the image of shape `[batch_size, channels, height, width]`\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion2\\model\\vae\\decoder.py:110\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m up \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup):\n\u001b[0;32m    108\u001b[0m     \u001b[39m# ResNet Blocks\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m up\u001b[39m.\u001b[39mblock:\n\u001b[1;32m--> 110\u001b[0m         h \u001b[39m=\u001b[39m block(h)\n\u001b[0;32m    111\u001b[0m     \u001b[39m# Up-sampling\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     h \u001b[39m=\u001b[39m up\u001b[39m.\u001b[39mupsample(h)\n",
      "File \u001b[1;32mc:\\Users\\igor-\\anaconda3\\envs\\kcg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion2\\model\\vae\\auxiliary_classes.py:165\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39m# Second normalization and convolution layer\u001b[39;00m\n\u001b[0;32m    164\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(h)\n\u001b[1;32m--> 165\u001b[0m h \u001b[39m=\u001b[39m swish(h)\n\u001b[0;32m    166\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(h)\n\u001b[0;32m    168\u001b[0m \u001b[39m# Map and add residual\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\stable_diffusion2\\model\\vae\\auxiliary_classes.py:178\u001b[0m, in \u001b[0;36mswish\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mswish\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    173\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    ### Swish activation\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[39m    $$x \\cdot \\sigma(x)$$\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49msigmoid(x)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 12.00 GiB total capacity; 10.95 GiB already allocated; 0 bytes free; 11.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "decoded_img = latent_diffusion_model.autoencoder_decode(encoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "base_directory = \"../\"\n",
    "sys.path.insert(0, base_directory)\n",
    "\n",
    "from stable_diffusion2.latent_diffusion import LatentDiffusion\n",
    "from stable_diffusion2.stable_diffusion import StableDiffusion\n",
    "from stable_diffusion2.utils.model import *\n",
    "from stable_diffusion2.utils.utils import SectionManager as section\n",
    "from stable_diffusion2.utils.utils import *\n",
    "from stable_diffusion2.model.clip.clip_embedder import CLIPTextEmbedder\n",
    "\n",
    "\n",
    "\n",
    "from stable_diffusion2.model.unet.unet import UNetModel\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: `device` is None. Falling back to current device.\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Starting section: encoder initialization...\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Finished section: encoder initialization in 0.27 seconds\n",
      "\n",
      "Starting section: decoder initialization...\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Finished section: decoder initialization in 0.27 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = check_device()\n",
    "encoder = initialize_encoder(device = device)\n",
    "decoder = initialize_decoder(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 10730 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1557 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(10730), tensor(12287))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting section: autoencoder model loading, from c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\autoencoder.ckpt...\n",
      "INFO: Using CUDA device 0: NVIDIA GeForce RTX 3080 Ti.\n",
      "Finished section: autoencoder model loading, from c:\\Users\\igor-\\.cloned\\kcg-ml-sd1p4\\input\\model\\autoencoder\\autoencoder.ckpt in 0.00 seconds\n",
      "\n",
      "Starting section: casting autoencoder model to device and evaling...\n",
      "Finished section: casting autoencoder model to device and evaling in 0.00 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_autoencoder(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 10730 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 1557 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(10730), tensor(12287))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.encoder = encoder\n",
    "autoencoder.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(\"./scripts2/test_img.jpg\").to(device)\n",
    "encoded_img = autoencoder.encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 1199 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 11088 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1199), tensor(12287))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = encoded_img.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free: 1260 MiB\n",
      "Total: 12287 MiB\n",
      "Used: 11027 MiB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(1260), tensor(12287))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autoencoder initialization from saved submodels...\n",
      "Finished autoencoder initialization from saved submodels in 0.25 seconds\n"
     ]
    }
   ],
   "source": [
    "with section(\"autoencoder initialization from saved submodels\"):\n",
    "    autoencoder_3 = Autoencoder(emb_channels=4, z_channels=4)\n",
    "    autoencoder_3.load_submodels(encoder_path=ENCODER_PATH, decoder_path=DECODER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_img_3 = autoencoder_3.encoder(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 64, 64]), torch.Size([1, 8, 64, 64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_img.shape, encoded_img_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(encoded_img_3 - encoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_diffusion_model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
